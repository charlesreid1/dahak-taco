include: "trimmed_data.settings"

import re
from os.path import join
from snakemake.remote.HTTP import RemoteProvider as HTTPRemoteProvider
HTTP = HTTPRemoteProvider()

data_dir = config['data_dir']

# trimmed data file name and OSF url
# come from trimmed_data.settings.
# incorporate OSF CLI here -
# user should just need project id.

trimmed_data_files=[]
trimmed_data_urls=[]
for filename in config['trimmed_data'].keys():
    trimmed_data_files.append( join(data_dir, filename) )
    trimmed_data_urls.append(config['trimmed_data'][filename])

rule download_trimmed_data:
    """
## Rule: `download_trimmed_data`

Download the trimmed data from OSF.
The trimmed data parameters 
consist of a dictionary of 
`*.fq.gz` files and corresponding URLs.

Simple workflow configuration:

```
$ cat get-trimmed-data.json
{
    'workflow_target' : 'download_trimmed_data'
}
```

Run the workflow:

```
$ ./taco -n get-trimmed-data
```

Run the workflow with custom, overriding parameters:

```
$ cat get-trimmed-data.json
{
    'workflow_target' : 'download_trimmed_data'
}

$ cat get-trimmed-data-params.json
{
    'trimmed_data' : {
        'filename.trim1.fq.gz' : <url-for-trim1>,
        'filename.trim2.fq.gz' : <url-for-trim2>
    }
}

$ ./taco -n get-trimmed-data get-trimmed-data-params
```

This rule is defined in `rules/dahak/trimmed_data.rule`.

This rule includes the files `trimmed_data.settings`.
    """
    input:
        HTTP.remote(expand("{url}", url=list(config['trimmed_data'].values())), keep_local=True)
    output:
        trimmed_data_files,
        touch(join(data_dir,'.trimmed'))
    run:
        for (osf_file,osf_url) in zip(trimmed_data_files,trimmed_data_urls):
            if(not os.path.isfile(osf_file)):
                shell('''
                    wget -O {osf_file} {osf_url}
                ''')


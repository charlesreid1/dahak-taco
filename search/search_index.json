{
    "docs": [
        {
            "location": "/",
            "text": "dahak-taco documentation\n\u00b6\n\n\ntaco\n is an experimental command-line interface\nfor running \ndahak\n\nworkflows using Snakemake.\n\n\n(insert icholy/ttygif here.)\n\n\nGetting Started\n\u00b6\n\n\ntaco\n is a lightweight wrapper around Snakemake tasks.\nKind of like a corn tortilla.\n\n\nThese sections will cover how to get up and running with \ntaco\n.\n\n\nInstallation\n - how to install \ntaco\n\n\nUsage\n - basic usage of \ntaco\n\n\nWorkflows\n - what are \ntaco\n workflows, \nhow do you run them, and what do they include?\n\n\nTests\n - how to run \ntaco\n unit tests \n\n\nWorkflows\n\u00b6\n\n\nThe actual taco workflow is defined using Snakemake rules,\nand these files should live in their own repository, separate\nfrom \ntaco\n.\n\n\nEach repository can define a single workflow, or multiple workflows.\n\n\nSeveral example \ntaco\n workflow repositories are available:\n\n\n\n\ntaco-simple\n - \n    illustrates the implementation of several \"hello world\" style taco workflows\n\n\ntaco-read-filtering\n - \n    implements a read filtering taco workflow.\n\n\ntaco-taxonomic-classification\n - \n    implements a taxonomic classification taco workflow.\n\n\n\n\nEach repository provides documentation and a Quick Start guide.\n\n\nCloud Platforms\n\u00b6\n\n\nWe include instructions for getting up and running with \ntaco\n on various cloud platforms.\nThis is a great application of the \ntaco-simple\n workflow.\n\n\n\n\nAWS Setup\n\n\nHPC - TBA\n\n\n\n\nConfiguration and Parameter Sets\n\u00b6\n\n\ntaco\n takes two input files: a workflow configuration file,\nwhich specifies the workflow targets, and a workflow\nparameters file, which specifies parameters to control\nthe workflow.\n\n\nWorkflow configuration and parameter files are workflow-dependent\nand live in workflow repositories.\n\n\nIndividual configuration files or parameter sets\nare workflow-dependent, and are defined or included\nin the repository that defines that workflow.\n\n\nFor a simple example of how configuration and parameter \nfiles are used, see the \ntaco-simple\n\nrepository, which contains several \"hello world\" style\n\ntaco\n workflows.\n\n\nAdvanced Topics\n\u00b6\n\n\nFor instructions on building, modifying, and improving\nthe documentation for \ntaco\n, see \n\nDocumentation\n.\n\n\nIf you are interested in creating a new workflow,\nsee \nDevWorkflows.md\n. Also see \nthe \ntaco-simple\n \nrepository, which illustrates simple \"hello world\" style \nworkflows using \ntaco\n best practices.\n\n\nFor more information about the development workflow,\nbranches, tags, and the release process, \nsee \nDevelopment\n.",
            "title": "Index"
        },
        {
            "location": "/#dahak-taco-documentation",
            "text": "taco  is an experimental command-line interface\nfor running  dahak \nworkflows using Snakemake.  (insert icholy/ttygif here.)",
            "title": "dahak-taco documentation"
        },
        {
            "location": "/#getting-started",
            "text": "taco  is a lightweight wrapper around Snakemake tasks.\nKind of like a corn tortilla.  These sections will cover how to get up and running with  taco .  Installation  - how to install  taco  Usage  - basic usage of  taco  Workflows  - what are  taco  workflows, \nhow do you run them, and what do they include?  Tests  - how to run  taco  unit tests",
            "title": "Getting Started"
        },
        {
            "location": "/#workflows",
            "text": "The actual taco workflow is defined using Snakemake rules,\nand these files should live in their own repository, separate\nfrom  taco .  Each repository can define a single workflow, or multiple workflows.  Several example  taco  workflow repositories are available:   taco-simple  - \n    illustrates the implementation of several \"hello world\" style taco workflows  taco-read-filtering  - \n    implements a read filtering taco workflow.  taco-taxonomic-classification  - \n    implements a taxonomic classification taco workflow.   Each repository provides documentation and a Quick Start guide.",
            "title": "Workflows"
        },
        {
            "location": "/#cloud-platforms",
            "text": "We include instructions for getting up and running with  taco  on various cloud platforms.\nThis is a great application of the  taco-simple  workflow.   AWS Setup  HPC - TBA",
            "title": "Cloud Platforms"
        },
        {
            "location": "/#configuration-and-parameter-sets",
            "text": "taco  takes two input files: a workflow configuration file,\nwhich specifies the workflow targets, and a workflow\nparameters file, which specifies parameters to control\nthe workflow.  Workflow configuration and parameter files are workflow-dependent\nand live in workflow repositories.  Individual configuration files or parameter sets\nare workflow-dependent, and are defined or included\nin the repository that defines that workflow.  For a simple example of how configuration and parameter \nfiles are used, see the  taco-simple \nrepository, which contains several \"hello world\" style taco  workflows.",
            "title": "Configuration and Parameter Sets"
        },
        {
            "location": "/#advanced-topics",
            "text": "For instructions on building, modifying, and improving\nthe documentation for  taco , see  Documentation .  If you are interested in creating a new workflow,\nsee  DevWorkflows.md . Also see \nthe  taco-simple  \nrepository, which illustrates simple \"hello world\" style \nworkflows using  taco  best practices.  For more information about the development workflow,\nbranches, tags, and the release process, \nsee  Development .",
            "title": "Advanced Topics"
        },
        {
            "location": "/Installation/",
            "text": "Installation\n\u00b6\n\n\ntaco\n is installed using \nsetup.py\n, and will be \ninstalled as a command-line utility on the system.\nWhen we are finished you will be able to run the \ncommand \n\n\n$ taco --help\n\n\n\n\n\nand see a \ntaco\n help message.\n\n\nClone\n\u00b6\n\n\nStart by cloning a local copy of the repository:\n\n\ngit clone https://github.com/dahak-metagenomics/dahak-taco\n\n\n\n\n\nYou can specify a particular tag or branch with the \nclone command, or use the default \nstable\n branch.\n\n\nBuild and Install\n\u00b6\n\n\nThe next step is to build and install the \ntaco\n source code\nusing \nsetup.py\n:\n\n\n$ python setup.py build\n\n\n\n\n\nIf you want to install taco as a system level module, run:\n\n\n$ python setup.py install\n\n\n\n\n\nIf you want to install it as a user (recommended):\n\n\n$ python setup.py install --user\n\n$ python setup.py install --user --prefix=    # <-- this is required on mac\n\n\n\n\n\nThis will install the taco command line utility as an \nexecutable program in the Python \nbin\n location. This \nlocation may or may not be on your path, and varies\ndepending on your Python environment, but should be in\n\n\n$PYTHONHOME/bin\n\n\n\n\n\nThis should be on your \n$PATH\n for you to be able to \ncall \ntaco\n from the command line.\n\n\nRun Tests\n\u00b6\n\n\nTo run tests:\n\n\npython setup.py test",
            "title": "Installation"
        },
        {
            "location": "/Installation/#installation",
            "text": "taco  is installed using  setup.py , and will be \ninstalled as a command-line utility on the system.\nWhen we are finished you will be able to run the \ncommand   $ taco --help  and see a  taco  help message.",
            "title": "Installation"
        },
        {
            "location": "/Installation/#clone",
            "text": "Start by cloning a local copy of the repository:  git clone https://github.com/dahak-metagenomics/dahak-taco  You can specify a particular tag or branch with the \nclone command, or use the default  stable  branch.",
            "title": "Clone"
        },
        {
            "location": "/Installation/#build-and-install",
            "text": "The next step is to build and install the  taco  source code\nusing  setup.py :  $ python setup.py build  If you want to install taco as a system level module, run:  $ python setup.py install  If you want to install it as a user (recommended):  $ python setup.py install --user\n\n$ python setup.py install --user --prefix=    # <-- this is required on mac  This will install the taco command line utility as an \nexecutable program in the Python  bin  location. This \nlocation may or may not be on your path, and varies\ndepending on your Python environment, but should be in  $PYTHONHOME/bin  This should be on your  $PATH  for you to be able to \ncall  taco  from the command line.",
            "title": "Build and Install"
        },
        {
            "location": "/Installation/#run-tests",
            "text": "To run tests:  python setup.py test",
            "title": "Run Tests"
        },
        {
            "location": "/Usage/",
            "text": "Usage\n\u00b6\n\n\nThe basic idea behind \ntaco\n is to pass it a verb, and \nmodify the interpretation of the verb using command line flags.\n\n\nThe user will defines their Snakemake workflows in a way that \nkeeps them general and utilizes user-provided parameters. \n\ntaco\n then reads these workflows, determines what rules exist\nand what rules were specified by the user, and runs the workflows\nusing the user-provided parameters.\n\n\nRemember, you can always get help by running:\n\n\n$ taco --help\n\n\n\n\n\nVerbs\n\u00b6\n\n\nRun the taco command line tool like this:\n\n\n$ ./taco <verb> [--FLAGS]\n\n\n\n\n\ntaco has two main actions:\n\n\n\n\n\n\ntaco ls [<worfklow>]\n - lists the available workflows, or the available rules in a given workflow\n\n\n\n\n\n\ntaco <worfklow>\n - runs the specified workflow\n\n\n\n\n\n\nEach workflow must specify a set of \nworkflow configuration variables \n(names of files or Snakemake rules to run)\nand workflow parameters (parameters used \nby the workflows themselves). These control\nthe details of the workflow.\n\n\nFlags\n\u00b6\n\n\nThe principal way to modify taco workflows \nis to use external YAML or JSON files.\n\n\nTo specify the configuration file, which \ntells taco which files to create or which\nrules to run, use:\n\n\n\n\n(--config-json | --config-yaml)\n - flags specifying \n    the path to the JSON or YAML file to use for the \n    workflow configuration.\n\n\n\n\nTo specify the parameters file, which \ncontrols the settings and details of \neach workflow step, use:\n\n\n\n\n(--params-json | --params-yaml)\n - flags specifying \n    the path to the JSON or YAML file to use for the \n    workflow parameters.\n\n\n\n\nPaths\n\u00b6\n\n\nBy default, \ntaco\n will create a working directory \ncalled \ndata/\n in your current working directory,\nand it will put all generated files in that directory.\n\n\nYou can change this location using the \n--prefix\n \ncommand line option. For example, to make a big mess\nin the current directory, you could leave prefix empty\nand run taco like this:\n\n\ntaco --prefix= <rest of args>\n\n\n\n\n\nYou can specify relative paths:\n\n\ntaco --prefix=scratch/ <rest of args>\n\n\n\n\n\nor absolute paths:\n\n\ntaco --prefix=/tmp <rest of args>",
            "title": "Usage"
        },
        {
            "location": "/Usage/#usage",
            "text": "The basic idea behind  taco  is to pass it a verb, and \nmodify the interpretation of the verb using command line flags.  The user will defines their Snakemake workflows in a way that \nkeeps them general and utilizes user-provided parameters.  taco  then reads these workflows, determines what rules exist\nand what rules were specified by the user, and runs the workflows\nusing the user-provided parameters.  Remember, you can always get help by running:  $ taco --help",
            "title": "Usage"
        },
        {
            "location": "/Usage/#verbs",
            "text": "Run the taco command line tool like this:  $ ./taco <verb> [--FLAGS]  taco has two main actions:    taco ls [<worfklow>]  - lists the available workflows, or the available rules in a given workflow    taco <worfklow>  - runs the specified workflow    Each workflow must specify a set of \nworkflow configuration variables \n(names of files or Snakemake rules to run)\nand workflow parameters (parameters used \nby the workflows themselves). These control\nthe details of the workflow.",
            "title": "Verbs"
        },
        {
            "location": "/Usage/#flags",
            "text": "The principal way to modify taco workflows \nis to use external YAML or JSON files.  To specify the configuration file, which \ntells taco which files to create or which\nrules to run, use:   (--config-json | --config-yaml)  - flags specifying \n    the path to the JSON or YAML file to use for the \n    workflow configuration.   To specify the parameters file, which \ncontrols the settings and details of \neach workflow step, use:   (--params-json | --params-yaml)  - flags specifying \n    the path to the JSON or YAML file to use for the \n    workflow parameters.",
            "title": "Flags"
        },
        {
            "location": "/Usage/#paths",
            "text": "By default,  taco  will create a working directory \ncalled  data/  in your current working directory,\nand it will put all generated files in that directory.  You can change this location using the  --prefix  \ncommand line option. For example, to make a big mess\nin the current directory, you could leave prefix empty\nand run taco like this:  taco --prefix= <rest of args>  You can specify relative paths:  taco --prefix=scratch/ <rest of args>  or absolute paths:  taco --prefix=/tmp <rest of args>",
            "title": "Paths"
        },
        {
            "location": "/Workflows/",
            "text": "Workflows\n\u00b6\n\n\nWorkflow Repositories\n\u00b6\n\n\nTo create a new workflow or set of workflows, \ncreate a new repository to hold all the files \nfor the workflow.\n\n\nYou will need a \nrules/\n folder, as well as a folder for\nconfiguration files and parameter files.\n\n\nThe taco tool is then run from that directory, and runs\nthe workflows defined in that directory.\n\n\nHere is an example repository layout for \na repository containing two related workflows:\n\n\ntaco-my-awesome-workflow/\n\n        rules/\n            workflowA/\n                Snakefile\n                purple.rule\n                blue.rule\n                green.rule\n                workflowA.settings\n\n            workflowB/\n                Snakefile\n                apple.rule\n                orange.rule\n                banana.rule\n                workflowB.settings\n\n        workflow-config/\n            config_make_apples.json\n            config_make_blue_apples.json\n            config_make_bananas.json\n            config_make_green_bananas.json\n\n        workflow-params/\n            params_lite.json\n            params_medium.json\n            params_heavy.json\n\n        docker/\n            utility_one/\n                Dockerfile\n            utility_two/\n                Dockerfile\n            utility_three/\n                Dockerfile\n\n\n\n\n\nEach of these components is discussed in more detail below,\nbut first we cover how to get up and running with an existing\nworkflow.\n\n\nRunning a Workflow\n\u00b6\n\n\nWhen you run \ntaco\n you can specify several verbs.\nThe \nls\n verb will list available workflows, which it \ndoes by looking for folders in the \nrules/\n directory\n(relative to the current directory where \ntaco\n was run)\nthat contain Snakemake workflows:\n\n\ntaco ls\n\n\n\n\n\nTo list rules in a particular workflow, give the name of the \nworkflow to the \nls\n verb:\n\n\ntaco ls <workflow-name>\n\n\n\n\n\nTo run the workflow, use the workflow name as the verb:\n\n\ntaco <workflow-name> [OPTIONS]\n\n\n\n\n\nWhen this command is executed, \ntaco\n loads the \n\nSnakefile\n (and subsequently all workflow rules)\nin the \nrules/<workflow-name>/\n directory.\nThe Snakefile will first load the user-provided\nworkflow parameters, then fill in any parameters\nthe user did not provide with the default values.\n\n\ntaco\n then uses the configuration file provided in \nthe options to determine what targets to make,\nand from that determines what rules to run.\n\n\nWorkflow Components\n\u00b6\n\n\nAll workflow repositories must include:\n\n\n\n\nRules\n\n\nWorkflow configuration files\n\n\nWorkflow parameter files\n\n\nDocumentation\n\n\n\n\nThe rules are required to define the workflow,\nand the configuration and parameter files are \nrequired to guide the user on how to use the \nworkflow.\n\n\nOptionally, if your workflow requires it you can\nalso include:\n\n\n\n\nDocker images\n\n\n\n\nRules\n\u00b6\n\n\nWorkflows are defined in a folder called \nrules/\n.\n\n\nThe \nrules/\n directory contains one folder per workflow.\nEach workflow is defined by \nrules/<workflow-name>/Snakefile\n,\nand workflows should include the following:\n\n\n\n\nSnakefile\n - this will consist mainly of \ninclude:\n statements\n\n\n*.rule\n files - these define the Snakemake rules\n\n\n*.settings\n files - usually one per workflow, these define default values \n    for all workflow parameters (and also serve as documentation for workflow\n    parameter values)\n\n\n\n\nThe user should specify the workflow name as the first\nverb on the command line, then pass any flags that modify\nthe workflow behavior. For example:\n\n\n$ taco read_filtering (--config-json|--config-yaml) (--params-json|--params-yaml)\n\n\n\n\n\n(More on the config and params flags below.)\n\n\nThis command will run the Snakemake API with the Snakefile found in \nrules/read_filtering\n \n(relative to the current directory where \ntaco\n was run). It uses the target specified by \nthe user's workflow configuration file to figure out what rules to run, and runs them using\nthe parameters in the user's parameters file.\n\n\nWorkflow Configuration File\n\u00b6\n\n\nThe workflow configuration file \ncontains a dictionary of configuration values.\n\n\nThe workflow configuration file\nmust specify a list of workflow targets\nassigned to the key \nworkflow_targets\n.\n\n\nThe value must be a valid Snakemake rule\ndefined in one of the rule files at \nrules/<workflow-name>/*.rules\n.\n\n\nThe form of the workflow configuration file \nfor running rules that have no file targets is\nto specify the name of the rule:\n\n\nJSON:\n\n\n{\n    \"workflow_targets\" : <list of Snakemake rules>\n}\n\n\n{\n    \"workflow_targets\" : <list of output file targets>\n}\n\n\n\n\n\nYAML:\n\n\nworkflow_targets:\n  - list\n  - of \n  - target\n  - files\n\n-----------\n\nworkflow_targets:\n  - list\n  - of\n  - target\n  - rules\n\n\n\n\n\nExample Rule-Based Workflow Configuration File\n\u00b6\n\n\nHere is a simple example of a workflow configuration file\nfor running a rule that has no file targets (pulling containers).\nThis rule specifies the workflow target by specifying the \nrule name:\n\n\nJSON:\n\n\n{\n    \"workflow_targets\" : [\"pull_biocontainers\"]\n}\n\n\n\n\n\nYAML:\n\n\nworkflow_targets:\n  - pull_biocontainers\n\n\n\n\n\nExample File-Based Workflow Configuration File\n\u00b6\n\n\nHere is a simple example of a workflow configuration file\nthat runs rules based on filename targets (in this example,\nthe output of a trimming process):\n\n\nJSON:\n\n\n{\n    \"workflow_targets\" : [\"XXXXXX_trim5_1.fq.gz\",\n                          \"XXXXXX_trim5_2.fq.gz\"]\n}\n\n\n\n\n\nYAML:\n\n\nworkflow_targets: \n  - XXXXXX_trim5_1.fq.gz\n  - XXXXXX_trim5_2.fq.gz\n\n\n\n\n\nWorkflow Parameters File\n\u00b6\n\n\nThe workflow parameters control various parameters\nthat are used by the programs and containers run\nby the Snakemake rules.\n\n\nEach workflow defines a set of default parameter values\nin a \n.settings\n file that lives next to the \nSnakefile\n\nand \n*.rule\n files.\n\n\nHowever, the user can set these parameter values in the \nworkflow parameters file, and these values set by the user \nwill override the default values.\n\n\nThe workflow parameters control how the workflow proceeds.\n\n\nThe workflow configuration controls the starting and ending points.\n\n\nThe form of the workflow parameters JSON file is:\n\n\nJSON:\n\n\n{\n    '<workflow-name>' : {\n        '<rule-name>' : {\n            '<param-name>' : <param-value>,\n            '<param-list>' : [<value1>, ...],\n            ...\n        }\n    }\n}\n\n\n\n\n\nYAML:\n\n\nmy_workflow_name:\n\n  my_rule_name_1:\n    my_param_name_1: \"my_param_value\"\n    my_param_name_2: 10\n\n  my_rule_name_2:\n    my_param_name_3: \"another_param_value\"\n    my_param_name_4: 100\n\n\n\n\n\nMost parameters are listed under the name of the application\nthat uses it, but some parameters have more generic names because\nthey are common to multiple tasks.\n\n\nExample Workflow Parameters File\n\u00b6\n\n\nThe following example parameters file adjusts the parameters for \nthe rule to update biocontainers.\n\n\nThis is a simple parameter file overriding a single parameter,\nthe version of sourmash being utilized.\n\n\nThis overrides the default biocontainers setting of \nsourmash version \n2.0.0a3--py36_0\n, set in \n\nrules/dahak/biocontainers.settings\n.\n\n\nJSON:\n\n\n{\n    \"biocontainers\" : {\n        \"trimmomatic\" : {\n            \"use_local\" : false,\n            \"quayurl\" : \"quay.io/biocontainers/trimmomatic\",\n            \"version\" : \"0.36--5\"\n        }\n    }\n}\n\n\n\n\n\nYAML:\n\n\nbiocontainers:\n  trimmomatic:\n    quayurl: \"quay.io/biocontainers/trimmomatic\"\n    version: '0.36--5\"\n    use_local: false\n\n\n\n\n\nDocker and Singularity\n\u00b6\n\n\nDocker Images\n\u00b6\n\n\nMost taco workflows utilize singularity to run docker containers\nas part of the workflows. Docker containers are usually specified\nby URL, but if a biocontainer or Dockerhub image is broken, a local\nimage must be used. \n\n\nFor this reason, some workflows will include their own\nDockerfiles. This is not recommended, but serves as a temporary fix.\n\n\nTo use a custom docker image in a taco workflow,\nadd a \ndocker\n directory, and add a subdirectory\nfor the utility you are adding. For example, if\nwe are adding a simple alpine widget called \nacme_widget\n,\nwe would create \ndocker/acme_widget/\n and add the \nDockerfile to \ndocker/acme_widget/Dockerfile\n.\n\n\nWe must also set up the configuration files and \nparameters to utilize local docker images.\nThis is largely workflow dependent, but \na good pattern to implement this is to include\na configuration dictionary for each application\n(such as \nacme_widget\n) that specifies whether \nto use a dockerhub/quay.io container image \nor a local container image:\n\n\nacme_widget:\n  use_local: true\n  local: my_acme_widget_image\n\n\n\n\n\nThis assumes you ran \n\n\ndocker build -t my_acme_widget_image .\n\n\n\n\n\nin the \ndocker/acme_widget/Dockerfile\n directory.\n\n\nSingularity\n\u00b6\n\n\n(Notes on singularity?)",
            "title": "Workflows"
        },
        {
            "location": "/Workflows/#workflows",
            "text": "",
            "title": "Workflows"
        },
        {
            "location": "/Workflows/#workflow-repositories",
            "text": "To create a new workflow or set of workflows, \ncreate a new repository to hold all the files \nfor the workflow.  You will need a  rules/  folder, as well as a folder for\nconfiguration files and parameter files.  The taco tool is then run from that directory, and runs\nthe workflows defined in that directory.  Here is an example repository layout for \na repository containing two related workflows:  taco-my-awesome-workflow/\n\n        rules/\n            workflowA/\n                Snakefile\n                purple.rule\n                blue.rule\n                green.rule\n                workflowA.settings\n\n            workflowB/\n                Snakefile\n                apple.rule\n                orange.rule\n                banana.rule\n                workflowB.settings\n\n        workflow-config/\n            config_make_apples.json\n            config_make_blue_apples.json\n            config_make_bananas.json\n            config_make_green_bananas.json\n\n        workflow-params/\n            params_lite.json\n            params_medium.json\n            params_heavy.json\n\n        docker/\n            utility_one/\n                Dockerfile\n            utility_two/\n                Dockerfile\n            utility_three/\n                Dockerfile  Each of these components is discussed in more detail below,\nbut first we cover how to get up and running with an existing\nworkflow.",
            "title": "Workflow Repositories"
        },
        {
            "location": "/Workflows/#running-a-workflow",
            "text": "When you run  taco  you can specify several verbs.\nThe  ls  verb will list available workflows, which it \ndoes by looking for folders in the  rules/  directory\n(relative to the current directory where  taco  was run)\nthat contain Snakemake workflows:  taco ls  To list rules in a particular workflow, give the name of the \nworkflow to the  ls  verb:  taco ls <workflow-name>  To run the workflow, use the workflow name as the verb:  taco <workflow-name> [OPTIONS]  When this command is executed,  taco  loads the  Snakefile  (and subsequently all workflow rules)\nin the  rules/<workflow-name>/  directory.\nThe Snakefile will first load the user-provided\nworkflow parameters, then fill in any parameters\nthe user did not provide with the default values.  taco  then uses the configuration file provided in \nthe options to determine what targets to make,\nand from that determines what rules to run.",
            "title": "Running a Workflow"
        },
        {
            "location": "/Workflows/#workflow-components",
            "text": "All workflow repositories must include:   Rules  Workflow configuration files  Workflow parameter files  Documentation   The rules are required to define the workflow,\nand the configuration and parameter files are \nrequired to guide the user on how to use the \nworkflow.  Optionally, if your workflow requires it you can\nalso include:   Docker images",
            "title": "Workflow Components"
        },
        {
            "location": "/Workflows/#rules",
            "text": "Workflows are defined in a folder called  rules/ .  The  rules/  directory contains one folder per workflow.\nEach workflow is defined by  rules/<workflow-name>/Snakefile ,\nand workflows should include the following:   Snakefile  - this will consist mainly of  include:  statements  *.rule  files - these define the Snakemake rules  *.settings  files - usually one per workflow, these define default values \n    for all workflow parameters (and also serve as documentation for workflow\n    parameter values)   The user should specify the workflow name as the first\nverb on the command line, then pass any flags that modify\nthe workflow behavior. For example:  $ taco read_filtering (--config-json|--config-yaml) (--params-json|--params-yaml)  (More on the config and params flags below.)  This command will run the Snakemake API with the Snakefile found in  rules/read_filtering  \n(relative to the current directory where  taco  was run). It uses the target specified by \nthe user's workflow configuration file to figure out what rules to run, and runs them using\nthe parameters in the user's parameters file.",
            "title": "Rules"
        },
        {
            "location": "/Workflows/#workflow-configuration-file",
            "text": "The workflow configuration file \ncontains a dictionary of configuration values.  The workflow configuration file\nmust specify a list of workflow targets\nassigned to the key  workflow_targets .  The value must be a valid Snakemake rule\ndefined in one of the rule files at  rules/<workflow-name>/*.rules .  The form of the workflow configuration file \nfor running rules that have no file targets is\nto specify the name of the rule:  JSON:  {\n    \"workflow_targets\" : <list of Snakemake rules>\n}\n\n\n{\n    \"workflow_targets\" : <list of output file targets>\n}  YAML:  workflow_targets:\n  - list\n  - of \n  - target\n  - files\n\n-----------\n\nworkflow_targets:\n  - list\n  - of\n  - target\n  - rules",
            "title": "Workflow Configuration File"
        },
        {
            "location": "/Workflows/#example-rule-based-workflow-configuration-file",
            "text": "Here is a simple example of a workflow configuration file\nfor running a rule that has no file targets (pulling containers).\nThis rule specifies the workflow target by specifying the \nrule name:  JSON:  {\n    \"workflow_targets\" : [\"pull_biocontainers\"]\n}  YAML:  workflow_targets:\n  - pull_biocontainers",
            "title": "Example Rule-Based Workflow Configuration File"
        },
        {
            "location": "/Workflows/#example-file-based-workflow-configuration-file",
            "text": "Here is a simple example of a workflow configuration file\nthat runs rules based on filename targets (in this example,\nthe output of a trimming process):  JSON:  {\n    \"workflow_targets\" : [\"XXXXXX_trim5_1.fq.gz\",\n                          \"XXXXXX_trim5_2.fq.gz\"]\n}  YAML:  workflow_targets: \n  - XXXXXX_trim5_1.fq.gz\n  - XXXXXX_trim5_2.fq.gz",
            "title": "Example File-Based Workflow Configuration File"
        },
        {
            "location": "/Workflows/#workflow-parameters-file",
            "text": "The workflow parameters control various parameters\nthat are used by the programs and containers run\nby the Snakemake rules.  Each workflow defines a set of default parameter values\nin a  .settings  file that lives next to the  Snakefile \nand  *.rule  files.  However, the user can set these parameter values in the \nworkflow parameters file, and these values set by the user \nwill override the default values.  The workflow parameters control how the workflow proceeds.  The workflow configuration controls the starting and ending points.  The form of the workflow parameters JSON file is:  JSON:  {\n    '<workflow-name>' : {\n        '<rule-name>' : {\n            '<param-name>' : <param-value>,\n            '<param-list>' : [<value1>, ...],\n            ...\n        }\n    }\n}  YAML:  my_workflow_name:\n\n  my_rule_name_1:\n    my_param_name_1: \"my_param_value\"\n    my_param_name_2: 10\n\n  my_rule_name_2:\n    my_param_name_3: \"another_param_value\"\n    my_param_name_4: 100  Most parameters are listed under the name of the application\nthat uses it, but some parameters have more generic names because\nthey are common to multiple tasks.",
            "title": "Workflow Parameters File"
        },
        {
            "location": "/Workflows/#example-workflow-parameters-file",
            "text": "The following example parameters file adjusts the parameters for \nthe rule to update biocontainers.  This is a simple parameter file overriding a single parameter,\nthe version of sourmash being utilized.  This overrides the default biocontainers setting of \nsourmash version  2.0.0a3--py36_0 , set in  rules/dahak/biocontainers.settings .  JSON:  {\n    \"biocontainers\" : {\n        \"trimmomatic\" : {\n            \"use_local\" : false,\n            \"quayurl\" : \"quay.io/biocontainers/trimmomatic\",\n            \"version\" : \"0.36--5\"\n        }\n    }\n}  YAML:  biocontainers:\n  trimmomatic:\n    quayurl: \"quay.io/biocontainers/trimmomatic\"\n    version: '0.36--5\"\n    use_local: false",
            "title": "Example Workflow Parameters File"
        },
        {
            "location": "/Workflows/#docker-and-singularity",
            "text": "",
            "title": "Docker and Singularity"
        },
        {
            "location": "/Workflows/#docker-images",
            "text": "Most taco workflows utilize singularity to run docker containers\nas part of the workflows. Docker containers are usually specified\nby URL, but if a biocontainer or Dockerhub image is broken, a local\nimage must be used.   For this reason, some workflows will include their own\nDockerfiles. This is not recommended, but serves as a temporary fix.  To use a custom docker image in a taco workflow,\nadd a  docker  directory, and add a subdirectory\nfor the utility you are adding. For example, if\nwe are adding a simple alpine widget called  acme_widget ,\nwe would create  docker/acme_widget/  and add the \nDockerfile to  docker/acme_widget/Dockerfile .  We must also set up the configuration files and \nparameters to utilize local docker images.\nThis is largely workflow dependent, but \na good pattern to implement this is to include\na configuration dictionary for each application\n(such as  acme_widget ) that specifies whether \nto use a dockerhub/quay.io container image \nor a local container image:  acme_widget:\n  use_local: true\n  local: my_acme_widget_image  This assumes you ran   docker build -t my_acme_widget_image .  in the  docker/acme_widget/Dockerfile  directory.",
            "title": "Docker Images"
        },
        {
            "location": "/Workflows/#singularity",
            "text": "(Notes on singularity?)",
            "title": "Singularity"
        },
        {
            "location": "/Tests/",
            "text": "Tests\n\u00b6\n\n\ntaco\n tests use the \nunittest\n module\n(same underlying test framework that nose uses,\nbut without the nose wrapper).\n\n\nAll tests are located in the \ntests/\n directory\nof the repository. They can be run through \nsetup.py\n:\n\n\npython setup.py test\n\n\n\n\n\nThe scope of the \ntaco\n unit tests is to focus on \ntesting basic functionality in taco - mainly checking\nthe use of different verbs and command line flags.",
            "title": "Tests"
        },
        {
            "location": "/Tests/#tests",
            "text": "taco  tests use the  unittest  module\n(same underlying test framework that nose uses,\nbut without the nose wrapper).  All tests are located in the  tests/  directory\nof the repository. They can be run through  setup.py :  python setup.py test  The scope of the  taco  unit tests is to focus on \ntesting basic functionality in taco - mainly checking\nthe use of different verbs and command line flags.",
            "title": "Tests"
        },
        {
            "location": "/AWSSetup/",
            "text": "AWS Setup for Walkthroughs\n\u00b6\n\n\nThis workflow covers getting set up with \nworker nodes, if you don't already have a\ncluster ready to go. \n\n\nIn this document we will:\n\n\n\n\nPrepare a beefy worker node to run a \n    dahak workflow by installing required software \n    from the dahak-yeti repository.\n\n\n\n\nWe will cover AWS in this document, although \ntaco can be used with various HPC and cloud \nplatforms.\n\n\nConsole\n\u00b6\n\n\nLog into the AWS console and select EC2.  Select a region (\nus-west-2\n Oregon is good for spot instances).\n\n\nCreate Instance\n\u00b6\n\n\nUse the big blue button to create a new EC2 instance.\n\n\nAmazon Machine Image\n\u00b6\n\n\nSelect the stock Ubuntu 16.04 LTS image from the list of AMIs.\nThis has the AMI ID:\n\n\nami-4e79ed36\n\n\n\n\n\nNode Type\n\u00b6\n\n\nThe hardware depends highly on the workflow\n(more results soon from benchmarking of workflows),\nbut for all walkthroughs we utilize one of the following:\n\n\n\n\nm5.2xlarge\n (8 vCPUs, 32 GB RAM) \n\n\nm5.4xlarge\n (16 vCPUs, 64 GB RAM)\n\n\n\n\nConfiguring Instance Details\n\u00b6\n\n\nOn the configure instance details:\n\n\n\n\n\n\nCheck \"Request Spot Instances\" box and set your desired price\n\n\n\n\nTypical price for 2xlarge is 14 cents per hour\n\n\nTypical price for 4xlarge is 28 cents per hour\n\n\n\n\n\n\n\n\nClick the Advanced Details bar at the bottom\n\n\n\n\n\n\nCopy the following into the user data text box:\n\n\n\n\n\n\n1\n2\n#!/bin/bash\n\nbash <\n(\n curl https://raw.githubusercontent.com/charlesreid1/dahak-yeti/master/cloud_init/cloud_init.sh \n)\n\n\n\n\n\n\n\n\n\n\n\nThe pipe-to-bash one-liner will run the cloud init script \n    in the \ndahak-yeti\n repo\n\n\n\n\nVolumes\n\u00b6\n\n\nA 256 GB hard disk (EBS, the default) should be sufficient.\n\n\nLogging Into Instance\n\u00b6\n\n\nWhen you create the node it should set up a private key\nto use to SSH into the worker node.\n\n\nThe init script will add a few minutes to the worker node's \nstartup time. It will run scripts, install files, set configurations,\nand run a lot of magic.\n\n\nOnce the startup step has completed, you can SSH to the\nworker node, and you will have the following installed:\n\n\n\n\npyenv\n\n\nminiconda3-4.30 installed with pyenv\n\n\nsnakemake installed with conda\n\n\nopinionated dotfiles (\n.bashrc\n, \n.bash_profile\n, \n.vimrc\n, etc.)\n\n\na colorful pink prompt",
            "title": "AWS Setup for Walkthroughs"
        },
        {
            "location": "/AWSSetup/#aws-setup-for-walkthroughs",
            "text": "This workflow covers getting set up with \nworker nodes, if you don't already have a\ncluster ready to go.   In this document we will:   Prepare a beefy worker node to run a \n    dahak workflow by installing required software \n    from the dahak-yeti repository.   We will cover AWS in this document, although \ntaco can be used with various HPC and cloud \nplatforms.",
            "title": "AWS Setup for Walkthroughs"
        },
        {
            "location": "/AWSSetup/#console",
            "text": "Log into the AWS console and select EC2.  Select a region ( us-west-2  Oregon is good for spot instances).",
            "title": "Console"
        },
        {
            "location": "/AWSSetup/#create-instance",
            "text": "Use the big blue button to create a new EC2 instance.",
            "title": "Create Instance"
        },
        {
            "location": "/AWSSetup/#amazon-machine-image",
            "text": "Select the stock Ubuntu 16.04 LTS image from the list of AMIs.\nThis has the AMI ID:  ami-4e79ed36",
            "title": "Amazon Machine Image"
        },
        {
            "location": "/AWSSetup/#node-type",
            "text": "The hardware depends highly on the workflow\n(more results soon from benchmarking of workflows),\nbut for all walkthroughs we utilize one of the following:   m5.2xlarge  (8 vCPUs, 32 GB RAM)   m5.4xlarge  (16 vCPUs, 64 GB RAM)",
            "title": "Node Type"
        },
        {
            "location": "/AWSSetup/#configuring-instance-details",
            "text": "On the configure instance details:    Check \"Request Spot Instances\" box and set your desired price   Typical price for 2xlarge is 14 cents per hour  Typical price for 4xlarge is 28 cents per hour     Click the Advanced Details bar at the bottom    Copy the following into the user data text box:    1\n2 #!/bin/bash \nbash < (  curl https://raw.githubusercontent.com/charlesreid1/dahak-yeti/master/cloud_init/cloud_init.sh  )      The pipe-to-bash one-liner will run the cloud init script \n    in the  dahak-yeti  repo",
            "title": "Configuring Instance Details"
        },
        {
            "location": "/AWSSetup/#volumes",
            "text": "A 256 GB hard disk (EBS, the default) should be sufficient.",
            "title": "Volumes"
        },
        {
            "location": "/AWSSetup/#logging-into-instance",
            "text": "When you create the node it should set up a private key\nto use to SSH into the worker node.  The init script will add a few minutes to the worker node's \nstartup time. It will run scripts, install files, set configurations,\nand run a lot of magic.  Once the startup step has completed, you can SSH to the\nworker node, and you will have the following installed:   pyenv  miniconda3-4.30 installed with pyenv  snakemake installed with conda  opinionated dotfiles ( .bashrc ,  .bash_profile ,  .vimrc , etc.)  a colorful pink prompt",
            "title": "Logging Into Instance"
        },
        {
            "location": "/SnakemakeRules/",
            "text": "Snakemake Rules\n\u00b6\n\n\nA brief summary of how Snakemake rules are implemented and organized.\n\n\nWe follow the conventions set in the \nsnakemake-rules\n\nrepository, and groups all rules into \n.rule\n files and all parameters into\n\n.settings\n files.\n\n\nFor a couple of simple workflows illustrating\nhow to assemble Snakemake rules that \ntaco\n \ncan utilize (i.e., that uses information \nfrom the user's config and params file),\nsee the \nsimple-taco\n\ntaco workflow repository.\n\n\nWorkflows\n\u00b6\n\n\nThe workflows define a set of tasks.\nEach workflow is a sub-directory in the \nrules/\n \ndirectory.\n\n\nRule Files\n\u00b6\n\n\nRule files define how steps in the workflow proceed.\nThey preprocess configuration variables.\nRules are defined in \n.rule\n files\nin the workflow directory.\n\n\nDefault Parameter Files\n\u00b6\n\n\nThe default parameter dictionary is contained\nin a \n.settings\n file in the workflow directory.\nThese will \nNOT\n overwrite any parameters\nthat have already been set by the user in their\nJSON parameter file.\n\n\nValidation of Parameters\n\u00b6\n\n\nBecause we don't know what workflow is being run\nwhen we process the parameters, we can't do much\nparameter validation.\n\n\nCurrently, we do not throw an exception \nwhen a parameter is undefined by the user,\nwe either silently use the default value \n(default behavior) or we silently set to \nan empty string (using \n--clean\n or \n-c\n\nflag).",
            "title": "Snakemake Rules"
        },
        {
            "location": "/SnakemakeRules/#snakemake-rules",
            "text": "A brief summary of how Snakemake rules are implemented and organized.  We follow the conventions set in the  snakemake-rules \nrepository, and groups all rules into  .rule  files and all parameters into .settings  files.  For a couple of simple workflows illustrating\nhow to assemble Snakemake rules that  taco  \ncan utilize (i.e., that uses information \nfrom the user's config and params file),\nsee the  simple-taco \ntaco workflow repository.",
            "title": "Snakemake Rules"
        },
        {
            "location": "/SnakemakeRules/#workflows",
            "text": "The workflows define a set of tasks.\nEach workflow is a sub-directory in the  rules/  \ndirectory.",
            "title": "Workflows"
        },
        {
            "location": "/SnakemakeRules/#rule-files",
            "text": "Rule files define how steps in the workflow proceed.\nThey preprocess configuration variables.\nRules are defined in  .rule  files\nin the workflow directory.",
            "title": "Rule Files"
        },
        {
            "location": "/SnakemakeRules/#default-parameter-files",
            "text": "The default parameter dictionary is contained\nin a  .settings  file in the workflow directory.\nThese will  NOT  overwrite any parameters\nthat have already been set by the user in their\nJSON parameter file.",
            "title": "Default Parameter Files"
        },
        {
            "location": "/SnakemakeRules/#validation-of-parameters",
            "text": "Because we don't know what workflow is being run\nwhen we process the parameters, we can't do much\nparameter validation.  Currently, we do not throw an exception \nwhen a parameter is undefined by the user,\nwe either silently use the default value \n(default behavior) or we silently set to \nan empty string (using  --clean  or  -c \nflag).",
            "title": "Validation of Parameters"
        },
        {
            "location": "/DevWorkflows/",
            "text": "Adding New Taco Workflows\n\u00b6\n\n\nIf you wish to extend or modify taco by \nadding a new workflow, you can do so,\nbut it's important to plan it out first!\n\n\nEach Snakemake workflow is composed of \na set of loosely-interconnected rules \nsharing parameters and handling pieces \nof the whole workflow.\n\n\nThe workflow lives in a directory \nin \nrules\n that is named after the \nworkflow. For example, to add a new\nworkflow called \nfoobar\n:\n\n\nrules/\n    foobar/\n    taxonomic_classification/\n    read_filtering/\n    functional_annotation/\n    ...\n\n\n\n\n\nEach rule or group of rules lives in a \n.rule file. All rule files have access\nto the entire configuration dictionary,\nwhich stores all workflow parameters.\n\n\nSuppose our new workflow \nfoobar\n had \nthree distinct steps: \nbuz\n, \nfuz\n, and \nwuz\n.\nThen we would create a rule file for each,\nand the \nfoobar/\n rules directory structure\nwould be:\n\n\nrules/\n    foobar/\n        buz.rule\n        fuz.rule\n        wuz.rule\n        foobar.settings\n\n\n\n\n\nAdditionally, we need to set parameters for the workflow.\n\n\nEach workflow has access to the \"master\" Snakemake\nparameters dictionary. \n\n\nIn addition, each workflow defines its own parameters dictionary\nto store parameters specific to that workflow.\n\n\nThis parameters dictionary is stored under a key that is the name \nof the workflow.\n\n\nFor example, if each of our three workflow steps took \nparameters, here is how we would organize the \nworkflow's default parameters dictionary:\n\n\nJSON:\n\n\n{\n    ...\n\n    'foobar' : {\n\n        'buz' : {\n            'buz_param_1' : 1,\n            'buz_param_2' : 'alpha'\n        },\n\n        'fuz' : {\n            'fuz_param_1' : 50,\n            'fuz_param_2' : 51,\n            'fuz_param_3' : False\n        },\n\n        'wuz' : {\n            'wuz_param_1' : 9.99,\n            'wuz_param_2' : 9.98,\n            'wuz_param_3' : 9.97,\n            'wuz_param_4' : 9.96\n        }\n    }\n    ...\n}\n\n\n\n\n\nYAML:\n\n\nfoobar:\n  buz:\n    - buz_param_1: 1\n    - buz_param_2: \"alpha\"\n  fuz:\n    - fuz_param_1: 50\n    - fuz_param_2: 51\n    - fuz_param_3: false\n\n  wuz:\n    - wuz_param_1: 9.99\n    - wuz_param_2: 9.98\n    - wuz_param_3: 9.97\n    - wuz_param_4: 9.96\n\n\n\n\n\nThe \nfoobar.settings\n file must set this \nin a way that will not overwrite defaults;\nhence this business:\n\n\nfrom snakemake.utils import update_config\n\nif(not config['clean']):\n\n    # Note: don't include http:// or https://\n    config_default = { \n                        ... \n                     }\n\n    update_config(config_default, config)\n    config = config_default",
            "title": "Developing Workflows"
        },
        {
            "location": "/DevWorkflows/#adding-new-taco-workflows",
            "text": "If you wish to extend or modify taco by \nadding a new workflow, you can do so,\nbut it's important to plan it out first!  Each Snakemake workflow is composed of \na set of loosely-interconnected rules \nsharing parameters and handling pieces \nof the whole workflow.  The workflow lives in a directory \nin  rules  that is named after the \nworkflow. For example, to add a new\nworkflow called  foobar :  rules/\n    foobar/\n    taxonomic_classification/\n    read_filtering/\n    functional_annotation/\n    ...  Each rule or group of rules lives in a \n.rule file. All rule files have access\nto the entire configuration dictionary,\nwhich stores all workflow parameters.  Suppose our new workflow  foobar  had \nthree distinct steps:  buz ,  fuz , and  wuz .\nThen we would create a rule file for each,\nand the  foobar/  rules directory structure\nwould be:  rules/\n    foobar/\n        buz.rule\n        fuz.rule\n        wuz.rule\n        foobar.settings  Additionally, we need to set parameters for the workflow.  Each workflow has access to the \"master\" Snakemake\nparameters dictionary.   In addition, each workflow defines its own parameters dictionary\nto store parameters specific to that workflow.  This parameters dictionary is stored under a key that is the name \nof the workflow.  For example, if each of our three workflow steps took \nparameters, here is how we would organize the \nworkflow's default parameters dictionary:  JSON:  {\n    ...\n\n    'foobar' : {\n\n        'buz' : {\n            'buz_param_1' : 1,\n            'buz_param_2' : 'alpha'\n        },\n\n        'fuz' : {\n            'fuz_param_1' : 50,\n            'fuz_param_2' : 51,\n            'fuz_param_3' : False\n        },\n\n        'wuz' : {\n            'wuz_param_1' : 9.99,\n            'wuz_param_2' : 9.98,\n            'wuz_param_3' : 9.97,\n            'wuz_param_4' : 9.96\n        }\n    }\n    ...\n}  YAML:  foobar:\n  buz:\n    - buz_param_1: 1\n    - buz_param_2: \"alpha\"\n  fuz:\n    - fuz_param_1: 50\n    - fuz_param_2: 51\n    - fuz_param_3: false\n\n  wuz:\n    - wuz_param_1: 9.99\n    - wuz_param_2: 9.98\n    - wuz_param_3: 9.97\n    - wuz_param_4: 9.96  The  foobar.settings  file must set this \nin a way that will not overwrite defaults;\nhence this business:  from snakemake.utils import update_config\n\nif(not config['clean']):\n\n    # Note: don't include http:// or https://\n    config_default = { \n                        ... \n                     }\n\n    update_config(config_default, config)\n    config = config_default",
            "title": "Adding New Taco Workflows"
        },
        {
            "location": "/Documentation/",
            "text": "Documentation\n\u00b6\n\n\nThis document covers the \ntaco\n documentation,\nhow to improve it, how to build it, and how to \nupdate the Github Pages/ReadTheDocs pages for\n\ntaco\n documentation.\n\n\nBuilding\n\u00b6\n\n\nTo build the documentation, you will need \nmkdocs\n.\n\n\npip install mkdocs\n\n\n\n\n\nYou will also need to check out the \nmkdocs-material-dib\n submodule\nfor the mkdocs documentation theme. \n\n\nTo include submodules when running the clone command:\n\n\ngit clone --recursive <url>\n\n\n\n\n\nTo clone submodules in an existing repository, run:\n\n\ngit submodule update --init\n\n\n\n\n\nOnce that's finished, you can run the following command\nfrom the current directory to build the documentation:\n\n\nmkdocs build\n\n\n\n\n\nThis generates all static content in the \nsite/\n directory.\nTo serve the documentation using a simple local HTTP server,\nrun:\n\n\nmkdocs serve\n\n\n\n\n\nUpdating Github Pages\n\u00b6\n\n\nSee the \nSetting Up Push-to-Deploy on Github Pages\n\nsection of the \nmkdocs-material-dib\n\ndocumentation for instructions.",
            "title": "Documentation"
        },
        {
            "location": "/Documentation/#documentation",
            "text": "This document covers the  taco  documentation,\nhow to improve it, how to build it, and how to \nupdate the Github Pages/ReadTheDocs pages for taco  documentation.",
            "title": "Documentation"
        },
        {
            "location": "/Documentation/#building",
            "text": "To build the documentation, you will need  mkdocs .  pip install mkdocs  You will also need to check out the  mkdocs-material-dib  submodule\nfor the mkdocs documentation theme.   To include submodules when running the clone command:  git clone --recursive <url>  To clone submodules in an existing repository, run:  git submodule update --init  Once that's finished, you can run the following command\nfrom the current directory to build the documentation:  mkdocs build  This generates all static content in the  site/  directory.\nTo serve the documentation using a simple local HTTP server,\nrun:  mkdocs serve",
            "title": "Building"
        },
        {
            "location": "/Documentation/#updating-github-pages",
            "text": "See the  Setting Up Push-to-Deploy on Github Pages \nsection of the  mkdocs-material-dib \ndocumentation for instructions.",
            "title": "Updating Github Pages"
        },
        {
            "location": "/Development/",
            "text": "Development\n\u00b6\n\n\nBranches and Tags\n\u00b6\n\n\nThere are two main branches for development in taco:\nan active development branch that may break,\nand an infrequently updated stable branch that \npoints to the latest tested and verified version\nof taco.\n\n\nWe utilize a two-track convention with branches - \none development branch and one stable branch: \n\n\n\n\n\n\nmaster\n (unstable) - this points to the latest commit on the \n    development branch of taco\n\n\n\n\n\n\nstable\n (stable) - this points to the latest commit on the \n    stable branch of taco\n\n\n\n\n\n\nBranches for each release version also exist.\nWe create a branch for each version release \nto enable version-specific maintenance commits.\nRelease branches also follow the two-track method.\nRelease branches are named:\n\n\n\n\n\n\nrelease/v0.0\n (stable) - this contains all commits specific to\n    release version 0.0 of taco\n\n\n\n\n\n\nrelease/v0.0beta\n (unstable) - this contains unstable development code\n    for \npreparing\n for release version 0.0 of taco (beta version)\n\n\n\n\n\n\nThe release branches use only the major and minor numbers,\nbut tags are used for each major, minor, and patch number.\nThe convention for naming tags in taco is the letter \nv\n \nfollowed by the version (major, minor, and patch) \nv0.0.0\n.\n\n\nAll tags for a major and minor version X.Y will point to \ncommits on the \nrelease/vX.Y\n branch.\n\n\nWorkflow\n\u00b6\n\n\nThe normal workflow is to create a new branch \nto add features or fix features, and merge commits \nfrom that branch into \nmaster\n, then into \n\nstable\n, and eventually on into a versioned release.\n\n\nAdding New Features\n\u00b6\n\n\nTo add new features, create a new branch using the \nmaster\n \nbranch as a source, and make your commits on this branch.\nWhen you're ready, run tests on the branch to ensure the \nmodified code works as expected. Merge this code into\nthe \nmaster\n branch.\n\n\nOnce the code has been tested, checked for backwards \ncompatibility, linted, minted, stamped, licensed, etc., \nit can be merged into the \nstable\n branch.\n\n\nCreating New Version\n\u00b6\n\n\nWhen you have commits in \nstable\n and are ready to\nrelease them as a new version, create a new \npre-release branch \nrelease/v0.0beta\n from the \nappropriate \nstable\n commit.\n\n\n(If the features you want to add to a new version of taco \nare not in the \nstable\n branch yet, start by getting them \ninto \nstable\n.)\n\n\nYou should \nnot\n be doing regular development on the \nbeta branches, but some hot-fix or workaround commits \nmay be required, hence the unstable label. \n\n\nOnce \nrelease/v0.0beta\n has been tested and is ready\nfor release, create a new branch \nrelease/v0.0\n \nfor the stable release of version 0.0 of taco.",
            "title": "Snakemake Development"
        },
        {
            "location": "/Development/#development",
            "text": "",
            "title": "Development"
        },
        {
            "location": "/Development/#branches-and-tags",
            "text": "There are two main branches for development in taco:\nan active development branch that may break,\nand an infrequently updated stable branch that \npoints to the latest tested and verified version\nof taco.  We utilize a two-track convention with branches - \none development branch and one stable branch:     master  (unstable) - this points to the latest commit on the \n    development branch of taco    stable  (stable) - this points to the latest commit on the \n    stable branch of taco    Branches for each release version also exist.\nWe create a branch for each version release \nto enable version-specific maintenance commits.\nRelease branches also follow the two-track method.\nRelease branches are named:    release/v0.0  (stable) - this contains all commits specific to\n    release version 0.0 of taco    release/v0.0beta  (unstable) - this contains unstable development code\n    for  preparing  for release version 0.0 of taco (beta version)    The release branches use only the major and minor numbers,\nbut tags are used for each major, minor, and patch number.\nThe convention for naming tags in taco is the letter  v  \nfollowed by the version (major, minor, and patch)  v0.0.0 .  All tags for a major and minor version X.Y will point to \ncommits on the  release/vX.Y  branch.",
            "title": "Branches and Tags"
        },
        {
            "location": "/Development/#workflow",
            "text": "The normal workflow is to create a new branch \nto add features or fix features, and merge commits \nfrom that branch into  master , then into  stable , and eventually on into a versioned release.",
            "title": "Workflow"
        },
        {
            "location": "/Development/#adding-new-features",
            "text": "To add new features, create a new branch using the  master  \nbranch as a source, and make your commits on this branch.\nWhen you're ready, run tests on the branch to ensure the \nmodified code works as expected. Merge this code into\nthe  master  branch.  Once the code has been tested, checked for backwards \ncompatibility, linted, minted, stamped, licensed, etc., \nit can be merged into the  stable  branch.",
            "title": "Adding New Features"
        },
        {
            "location": "/Development/#creating-new-version",
            "text": "When you have commits in  stable  and are ready to\nrelease them as a new version, create a new \npre-release branch  release/v0.0beta  from the \nappropriate  stable  commit.  (If the features you want to add to a new version of taco \nare not in the  stable  branch yet, start by getting them \ninto  stable .)  You should  not  be doing regular development on the \nbeta branches, but some hot-fix or workaround commits \nmay be required, hence the unstable label.   Once  release/v0.0beta  has been tested and is ready\nfor release, create a new branch  release/v0.0  \nfor the stable release of version 0.0 of taco.",
            "title": "Creating New Version"
        }
    ]
}
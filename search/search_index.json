{
    "docs": [
        {
            "location": "/",
            "text": "dahak-taco documentation\n\n\ndahak-taco is an experimental command-line interface\nfor running dahak workflows using Snakemake.\n\n\n(insert icholy/ttygif here.)\n\n\nTo get started with taco,\nand run your first workflow task,\nsee the \nGetting Started\n Section below.\n\n\nIf you're hungry for more dahak workflows,\nskip to the \nWalkthroughs Section\n below.\n\n\nIf you are already up and running \nwith taco and are looking for \ninformation about the rules and their \nparameters, check out the \n\nAPI Section\n below.\n\n\nIf you are extending taco by adding new\nrules, workflows, or documentation, see \nthe \nDevelopers Section\n.\n\n\nGetting Started\n\n\ntaco is a command line utility that wraps\nSnakemake rules to run complex workflows.\n\n\nThese sections will cover what taco is\nand get you up and running with your \nfirst taco workflow.\n\n\nInstallation and Usage\n\n\nQuickstart\n\n\nWorkflow Walkthroughs\n\n\n(in progress)\n\n\ntaco is a lightweight wrapper around Snakemake tasks.\nKind of like a corn tortilla.\n\n\nIn the quickstart, we showed how to create a repository\nfor a set of workflows. Below, we link to repositories\nwith useful workflows and walkthroughs.\n\n\nAWS Setup\n\n\nRead Filtering Walkthrough\n\n\nConfiguration and Parameter Sets\n\n\n(in progress)\n\n\nIndividual configuration files or parameter sets\nare workflow-dependent, and are defined or included\nin the repository that defines that workflow.\n\n\nThe following are some example taco workflows\nwith configuration and parameter sets included:\n\n\n\n\ntaco-simple\n - illustrates\n    a simple workflow that prints messages\n\n\ntaco-read-filtering\n - illustrates\n    a read filtering workflow.\n\n\ntaco-taxonomic-classification\n - illustrates\n    a taxonomic classification workflow.\n\n\n\n\nFor Developers\n\n\n(in progress)\n\n\nDocumentation on these pages describe how taco works\nso that you can modify it to suit your needs. \n\n\nCurrently covered are snakemake rules and defining \nnew workflows.\n\n\nSnakemake Rules\n\n\nWorkflows\n\n\nLicense\n\n\nBSD 3-Clause License\n\n\nCopyright (c) 2018, Charles Reid\nAll rights reserved.\n\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n\n\n\n\n\nRedistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n\n\n\n\n\nRedistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\n\n\n\n\n\nNeither the name of the copyright holder nor the names of its\n  contributors may be used to endorse or promote products derived from\n  this software without specific prior written permission.\n\n\n\n\n\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.",
            "title": "Index"
        },
        {
            "location": "/#dahak-taco-documentation",
            "text": "dahak-taco is an experimental command-line interface\nfor running dahak workflows using Snakemake.  (insert icholy/ttygif here.)  To get started with taco,\nand run your first workflow task,\nsee the  Getting Started  Section below.  If you're hungry for more dahak workflows,\nskip to the  Walkthroughs Section  below.  If you are already up and running \nwith taco and are looking for \ninformation about the rules and their \nparameters, check out the  API Section  below.  If you are extending taco by adding new\nrules, workflows, or documentation, see \nthe  Developers Section .",
            "title": "dahak-taco documentation"
        },
        {
            "location": "/#getting-started",
            "text": "taco is a command line utility that wraps\nSnakemake rules to run complex workflows.  These sections will cover what taco is\nand get you up and running with your \nfirst taco workflow.  Installation and Usage  Quickstart",
            "title": "Getting Started"
        },
        {
            "location": "/#workflow-walkthroughs",
            "text": "(in progress)  taco is a lightweight wrapper around Snakemake tasks.\nKind of like a corn tortilla.  In the quickstart, we showed how to create a repository\nfor a set of workflows. Below, we link to repositories\nwith useful workflows and walkthroughs.  AWS Setup  Read Filtering Walkthrough",
            "title": "Workflow Walkthroughs"
        },
        {
            "location": "/#configuration-and-parameter-sets",
            "text": "(in progress)  Individual configuration files or parameter sets\nare workflow-dependent, and are defined or included\nin the repository that defines that workflow.  The following are some example taco workflows\nwith configuration and parameter sets included:   taco-simple  - illustrates\n    a simple workflow that prints messages  taco-read-filtering  - illustrates\n    a read filtering workflow.  taco-taxonomic-classification  - illustrates\n    a taxonomic classification workflow.",
            "title": "Configuration and Parameter Sets"
        },
        {
            "location": "/#for-developers",
            "text": "(in progress)  Documentation on these pages describe how taco works\nso that you can modify it to suit your needs.   Currently covered are snakemake rules and defining \nnew workflows.  Snakemake Rules  Workflows",
            "title": "For Developers"
        },
        {
            "location": "/#license",
            "text": "BSD 3-Clause License  Copyright (c) 2018, Charles Reid\nAll rights reserved.  Redistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:    Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.    Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.    Neither the name of the copyright holder nor the names of its\n  contributors may be used to endorse or promote products derived from\n  this software without specific prior written permission.    THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.",
            "title": "License"
        },
        {
            "location": "/InstallationUsage/",
            "text": "Installation\n\n\nThe taco utility is a command-line utility\nthat runs Snakemake workflows.\n\n\ntaco is installed using \nsetup.py\n and is installed\nas a command-line utility on the system:\n\n\npython setup.py build\n\npython setup.py install\n\n\n\n\nUsage\n\n\nThe basic idea behind taco is to \npass it an action and modify basic\nbehavior with command line flags.\n\n\nThe user will defines their Snakemake workflows\nin a way that keeps them general. Then taco can\nrun those workflows from the command line, and\nuse command line flags and external files to change\nworkflow targets and parameter sets. \n\n\nActions\n\n\nRun the taco command line tool like this:\n\n\n$ ./taco <action> --arguments\n\n\n\n\ntaco has two main actions:\n\n\ntaco ls [<worfklow>]\n - lists the available workflows and rules in workflows\n\n\ntaco <worfklow>\n - runs the specified workflow\n\n\nEach workflow must specify a set of \nworkflow configuration variables \n(names of files or Snakemake rules to run)\nand workflow parameters (parameters used \nby the workflows themselves). These control\nthe details of the workflow.\n\n\nFlags\n\n\nThe principal way to modify taco workflows \nis to use external YAML or JSON files.\n\n\nTo specify the configuration file, which \ntells taco which files to create or which\nrules to run, use:\n\n\n\n\n(--config-json | --config-yaml)\n - flags specifying \n    the path to the JSON or YAML file to use for the \n    workflow configuration.\n\n\n\n\nTo specify the parameters file, which \ncontrols the settings and details of \neach workflow step, use:\n\n\n\n\n(--params-json | --params-yaml)\n - flags specifying \n    the path to the JSON or YAML file to use for the \n    workflow parameters.\n\n\n\n\nRules\n\n\nWorkflows are defined in a folder called \nrules/\n.\n\n\nThe \nrules/\n directory contains one folder per workflow.\n\n\nEach workflow directory contains rules in \n*.rule\n files\nand default parameters in \n*.settings\n files, as well as \na Snakefile that is imported by taco and that should \nin turn include each rule and settings file used by \nthat workflow.\n\n\nDocker and Singularity\n\n\nMost taco workflows utilize singularity to run docker containers\nas part of the workflows. Docker containers are usually specified\nby URL, but if a biocontainer or Dockerhub image is broken, a local\nimage must be used. \n\n\nFor this reason, some workflows will include their own\nDockerfiles. This is not recommended and should serve \nonly as a temporary fix.\n\n\nWorkflow Repository\n\n\nTo create a new set of workflows, create a new repository.\n\n\nYou will need a \nrules/\n folder, as well as a folder for\nconfiguration files and parameter files.\n\n\nThe taco tool is then run from that directory, and runs\nthe workflows defined in that directory.\n\n\nHere is an example repository layout for \na repository containing two related workflows:\n\n\ntaco-my-cool-workflow/\n\n        rules/\n            workflow_A/\n                Snakefile\n                purple.rule\n                blue.rule\n                green.rule\n                workflow_A.settings\n\n            workflow_B/\n                Snakefile\n                apple.rule\n                orange.rule\n                banana.rule\n                workflow_B.settings\n\n        workflow-config/\n            config_make_apples.json\n            config_make_blue_apples.json\n            config_make_bananas.json\n            config_make_green_bananas.json\n\n        workflow-params/\n            params_lite.json\n            params_medium.json\n            params_heavy.json\n\n        docker/\n            utility_one/\n                Dockerfile\n            utility_two/\n                Dockerfile\n            utility_three/\n                Dockerfile\n\n\n\n\nWorkflow Name\n\n\nEach workflow is defined by a Snakefile that includes\na set of rules for each step of the workflow\nand a dictionary of parameter values. \n\n\nEach workflow is in its own directory in the \n\nrules/\n\ndirectory of this repository and must have \na Snakefile.\n\n\nThe user should specify the workflow name as the first\nverb on the command line. For example:\n\n\n$ taco read_filtering <workflow-config-file> <workflow-params-file>\n\n\n\n\nwill load the Snakefile in \nrules/read_filtering/\n \nand will make avaiable all read filtering rules.\n\n\nWorkflow Configuration File\n\n\nThe workflow configuration file \ncontains a dictionary of configuration values.\n\n\nThe workflow configuration file\nmust specify a list of workflow targets\nassigned to the key \nworkflow_targets\n.\n\n\nThe value must be a valid Snakemake rule\ndefined in one of the rule files at \nrules/<workflow-name>/*.rules\n.\n\n\nThe form of the workflow configuration file \nfor running rules that have no file targets is\nto specify the name of the rule:\n\n\n{\n    \"workflow_targets\" : <list of Snakemake rules>\n}\n\n\n\n\nor,\n\n\n{\n    \"workflow_targets\" : <list of output file targets>\n}\n\n\n\n\nExample Rule-Based Workflow Configuration File\n\n\nHere is a simple example of a workflow configuration file\nfor running a rule that has no file targets (pulling containers).\nThis rule specifies the workflow target by specifying the \nrule name:\n\n\ngoodies/test-conf.json\n:\n\n\n{\n    \"workflow_targets\" : [\"pull_biocontainers\"]\n}\n\n\n\n\nExample File-Based Workflow Configuration File\n\n\nHere is a simple example of a workflow configuration file\nthat runs rules based on filename targets (in this example,\nthe output of a trimming process):\n\n\ngoodies/test-conf.json\n:\n\n\n{\n    \"workflow_targets\" : [\"XXXXXX_trim5_1.fq.gz\",\n                          \"XXXXXX_trim5_2.fq.gz\"]\n}\n\n\n\n\nWorkflow Parameters File\n\n\nThe workflow parameters control various parameters\nthat are used by the programs and containers run\nby the Snakemake rules.\n\n\nEach workflow defines a set of default parameter values\nin a \n.settings\n file that lives next to the \nSnakefile\n\nand \n*.rule\n files.\n\n\nHowever, the user can set these parameter values in the \nworkflow parameters file, and these values set by the user \nwill override the default values.\n\n\nThe workflow parameters control how the workflow proceeds.\n\n\nThe workflow configuration controls the starting and ending points.\n\n\nThe form of the workflow parameters JSON file is:\n\n\n{\n    '<workflow-name>' : {\n        '<rule-name>' : {\n            '<param-name>' : <param-value>,\n            ...\n        }\n    }\n}\n\n\n\n\nMost parameters are listed under the name of the application\nthat uses it, but some parameters have more generic names because\nthey are common to multiple tasks.\n\n\nExample Workflow Parameters File\n\n\nThe following example parameters file adjusts the parameters for \nthe rule to update biocontainers.\n\n\nThis is a simple parameter file overriding a single parameter,\nthe version of sourmash being utilized.\n\n\nThis overrides the default biocontainers setting of \nsourmash version \n2.0.0a3--py36_0\n, set in \n\nrules/dahak/biocontainers.settings\n.\n\n\ngoodies/test-params.json\n:\n\n\n{\n    \"biocontainers\" : {\n        \"trimmomatic\" : {\n            \"use_local\" : false,\n            \"quayurl\" : \"quay.io/biocontainers/trimmomatic\",\n            \"version\" : \"0.36--5\"\n        }\n    }\n}\n\n\n\n\nRunning Taco\n\n\nTo run taco, pass it three arguments on the command line:\n\n\n\n\nName of the workflow\n\n\nRelative path to workflow configuration file\n\n\nRelative path to workflow parameters file\n\n\n\n\nBoth arguments should be the path to a JSON file\n(absolute or relative) and exclude the JSON suffix.\n\n\nRunning an Example\n\n\nTo run the two JSON files in the example above,\nwhich are contained in the \ngoodies/\n directory \nand which are the first steps in the taxonomic classification \nworkflow, we would run:\n\n\n$ ./taco taxonomic_classification goodies/test-conf.json goodies/test-params.json\n\n\n\n\n(The \n.json\n extension can be included or excluded.)\n\n\nTo do a dry run only, add the \n-n\n or \n--dry-run\n flag:\n\n\n$ ./taco -n taxonomic_classification goodies/test-conf goodies/test-params\n\n\n\n\nThis will run the \npull_biocontainers\n rule.\n\n\nListing Available Actions\n\n\nListing Workflows\n\n\nYou can list available taco workflows\nusing the \ntaco ls\n command with no \nadditional arguments:\n\n\n$ ./taco ls\n\n\n\n\nListing Workflow Rules\n\n\n(Not available)\n\n\nYou can list all of the rules available \nfor a given workflow by running \ntaco ls <workflow-name>\n.\nFor example:\n\n\n$ ./taco ls read_filtering",
            "title": "Installation and Usage"
        },
        {
            "location": "/InstallationUsage/#installation",
            "text": "The taco utility is a command-line utility\nthat runs Snakemake workflows.  taco is installed using  setup.py  and is installed\nas a command-line utility on the system:  python setup.py build\n\npython setup.py install",
            "title": "Installation"
        },
        {
            "location": "/InstallationUsage/#usage",
            "text": "The basic idea behind taco is to \npass it an action and modify basic\nbehavior with command line flags.  The user will defines their Snakemake workflows\nin a way that keeps them general. Then taco can\nrun those workflows from the command line, and\nuse command line flags and external files to change\nworkflow targets and parameter sets.",
            "title": "Usage"
        },
        {
            "location": "/InstallationUsage/#actions",
            "text": "Run the taco command line tool like this:  $ ./taco <action> --arguments  taco has two main actions:  taco ls [<worfklow>]  - lists the available workflows and rules in workflows  taco <worfklow>  - runs the specified workflow  Each workflow must specify a set of \nworkflow configuration variables \n(names of files or Snakemake rules to run)\nand workflow parameters (parameters used \nby the workflows themselves). These control\nthe details of the workflow.",
            "title": "Actions"
        },
        {
            "location": "/InstallationUsage/#flags",
            "text": "The principal way to modify taco workflows \nis to use external YAML or JSON files.  To specify the configuration file, which \ntells taco which files to create or which\nrules to run, use:   (--config-json | --config-yaml)  - flags specifying \n    the path to the JSON or YAML file to use for the \n    workflow configuration.   To specify the parameters file, which \ncontrols the settings and details of \neach workflow step, use:   (--params-json | --params-yaml)  - flags specifying \n    the path to the JSON or YAML file to use for the \n    workflow parameters.",
            "title": "Flags"
        },
        {
            "location": "/InstallationUsage/#rules",
            "text": "Workflows are defined in a folder called  rules/ .  The  rules/  directory contains one folder per workflow.  Each workflow directory contains rules in  *.rule  files\nand default parameters in  *.settings  files, as well as \na Snakefile that is imported by taco and that should \nin turn include each rule and settings file used by \nthat workflow.",
            "title": "Rules"
        },
        {
            "location": "/InstallationUsage/#docker-and-singularity",
            "text": "Most taco workflows utilize singularity to run docker containers\nas part of the workflows. Docker containers are usually specified\nby URL, but if a biocontainer or Dockerhub image is broken, a local\nimage must be used.   For this reason, some workflows will include their own\nDockerfiles. This is not recommended and should serve \nonly as a temporary fix.",
            "title": "Docker and Singularity"
        },
        {
            "location": "/InstallationUsage/#workflow-repository",
            "text": "To create a new set of workflows, create a new repository.  You will need a  rules/  folder, as well as a folder for\nconfiguration files and parameter files.  The taco tool is then run from that directory, and runs\nthe workflows defined in that directory.  Here is an example repository layout for \na repository containing two related workflows:  taco-my-cool-workflow/\n\n        rules/\n            workflow_A/\n                Snakefile\n                purple.rule\n                blue.rule\n                green.rule\n                workflow_A.settings\n\n            workflow_B/\n                Snakefile\n                apple.rule\n                orange.rule\n                banana.rule\n                workflow_B.settings\n\n        workflow-config/\n            config_make_apples.json\n            config_make_blue_apples.json\n            config_make_bananas.json\n            config_make_green_bananas.json\n\n        workflow-params/\n            params_lite.json\n            params_medium.json\n            params_heavy.json\n\n        docker/\n            utility_one/\n                Dockerfile\n            utility_two/\n                Dockerfile\n            utility_three/\n                Dockerfile",
            "title": "Workflow Repository"
        },
        {
            "location": "/InstallationUsage/#workflow-name",
            "text": "Each workflow is defined by a Snakefile that includes\na set of rules for each step of the workflow\nand a dictionary of parameter values.   Each workflow is in its own directory in the  rules/ \ndirectory of this repository and must have \na Snakefile.  The user should specify the workflow name as the first\nverb on the command line. For example:  $ taco read_filtering <workflow-config-file> <workflow-params-file>  will load the Snakefile in  rules/read_filtering/  \nand will make avaiable all read filtering rules.",
            "title": "Workflow Name"
        },
        {
            "location": "/InstallationUsage/#workflow-configuration-file",
            "text": "The workflow configuration file \ncontains a dictionary of configuration values.  The workflow configuration file\nmust specify a list of workflow targets\nassigned to the key  workflow_targets .  The value must be a valid Snakemake rule\ndefined in one of the rule files at  rules/<workflow-name>/*.rules .  The form of the workflow configuration file \nfor running rules that have no file targets is\nto specify the name of the rule:  {\n    \"workflow_targets\" : <list of Snakemake rules>\n}  or,  {\n    \"workflow_targets\" : <list of output file targets>\n}",
            "title": "Workflow Configuration File"
        },
        {
            "location": "/InstallationUsage/#example-rule-based-workflow-configuration-file",
            "text": "Here is a simple example of a workflow configuration file\nfor running a rule that has no file targets (pulling containers).\nThis rule specifies the workflow target by specifying the \nrule name:  goodies/test-conf.json :  {\n    \"workflow_targets\" : [\"pull_biocontainers\"]\n}",
            "title": "Example Rule-Based Workflow Configuration File"
        },
        {
            "location": "/InstallationUsage/#example-file-based-workflow-configuration-file",
            "text": "Here is a simple example of a workflow configuration file\nthat runs rules based on filename targets (in this example,\nthe output of a trimming process):  goodies/test-conf.json :  {\n    \"workflow_targets\" : [\"XXXXXX_trim5_1.fq.gz\",\n                          \"XXXXXX_trim5_2.fq.gz\"]\n}",
            "title": "Example File-Based Workflow Configuration File"
        },
        {
            "location": "/InstallationUsage/#workflow-parameters-file",
            "text": "The workflow parameters control various parameters\nthat are used by the programs and containers run\nby the Snakemake rules.  Each workflow defines a set of default parameter values\nin a  .settings  file that lives next to the  Snakefile \nand  *.rule  files.  However, the user can set these parameter values in the \nworkflow parameters file, and these values set by the user \nwill override the default values.  The workflow parameters control how the workflow proceeds.  The workflow configuration controls the starting and ending points.  The form of the workflow parameters JSON file is:  {\n    '<workflow-name>' : {\n        '<rule-name>' : {\n            '<param-name>' : <param-value>,\n            ...\n        }\n    }\n}  Most parameters are listed under the name of the application\nthat uses it, but some parameters have more generic names because\nthey are common to multiple tasks.",
            "title": "Workflow Parameters File"
        },
        {
            "location": "/InstallationUsage/#example-workflow-parameters-file",
            "text": "The following example parameters file adjusts the parameters for \nthe rule to update biocontainers.  This is a simple parameter file overriding a single parameter,\nthe version of sourmash being utilized.  This overrides the default biocontainers setting of \nsourmash version  2.0.0a3--py36_0 , set in  rules/dahak/biocontainers.settings .  goodies/test-params.json :  {\n    \"biocontainers\" : {\n        \"trimmomatic\" : {\n            \"use_local\" : false,\n            \"quayurl\" : \"quay.io/biocontainers/trimmomatic\",\n            \"version\" : \"0.36--5\"\n        }\n    }\n}",
            "title": "Example Workflow Parameters File"
        },
        {
            "location": "/InstallationUsage/#running-taco",
            "text": "To run taco, pass it three arguments on the command line:   Name of the workflow  Relative path to workflow configuration file  Relative path to workflow parameters file   Both arguments should be the path to a JSON file\n(absolute or relative) and exclude the JSON suffix.",
            "title": "Running Taco"
        },
        {
            "location": "/InstallationUsage/#running-an-example",
            "text": "To run the two JSON files in the example above,\nwhich are contained in the  goodies/  directory \nand which are the first steps in the taxonomic classification \nworkflow, we would run:  $ ./taco taxonomic_classification goodies/test-conf.json goodies/test-params.json  (The  .json  extension can be included or excluded.)  To do a dry run only, add the  -n  or  --dry-run  flag:  $ ./taco -n taxonomic_classification goodies/test-conf goodies/test-params  This will run the  pull_biocontainers  rule.",
            "title": "Running an Example"
        },
        {
            "location": "/InstallationUsage/#listing-available-actions",
            "text": "",
            "title": "Listing Available Actions"
        },
        {
            "location": "/InstallationUsage/#listing-workflows",
            "text": "You can list available taco workflows\nusing the  taco ls  command with no \nadditional arguments:  $ ./taco ls",
            "title": "Listing Workflows"
        },
        {
            "location": "/InstallationUsage/#listing-workflow-rules",
            "text": "(Not available)  You can list all of the rules available \nfor a given workflow by running  taco ls <workflow-name> .\nFor example:  $ ./taco ls read_filtering",
            "title": "Listing Workflow Rules"
        },
        {
            "location": "/Quickstart/",
            "text": "Quick Start\n\n\nThe basic idea behind taco is to \npass it an action and modify basic\nbehavior with command line flags.\n\n\n$ ./taco <action> --arguments\n\n\n\n\nIn this section we'll get started\nwith a simple workflow to illustrate\nhow to use taco.\n\n\nWorkflow Repository\n\n\nTo create a new set of workflows, create a new repository.\nTo illustrate how to run taco, we will use a single workflow \nwith two steps.\n\n\nHere is the repository layout for our simple example:\n\n\ntaco-simple/\n\n        rules/\n            my_simple_workflow/\n                Snakefile\n                step1.rule\n                workflow_A.settings\n\n        workflow-config/\n            configA.json\n            configB.json\n\n        workflow-params/\n            paramsA.json\n            paramsB.json\n\n\n\n\nWorkflow Name\n\n\nEach workflow is defined by a Snakefile that includes\na set of rules for each step of the workflow\nand a dictionary of parameter values. \n\n\nEach workflow is in its own directory in the \n\nrules/\n\ndirectory of this repository and must have \na Snakefile.\n\n\nThe user should specify the workflow name as the first\nverb on the command line. For example:\n\n\n$ taco read_filtering <workflow-config-file> <workflow-params-file>\n\n\n\n\nwill load the Snakefile in \nrules/read_filtering/\n \nand will make avaiable all read filtering rules.\n\n\nWorkflow Configuration File\n\n\nThe workflow configuration file \ncontains a dictionary of configuration values.\n\n\nThe workflow configuration file\nmust specify a list of workflow targets\nassigned to the key \nworkflow_targets\n.\n\n\nThe value must be a valid Snakemake rule\ndefined in one of the rule files at \nrules/<workflow-name>/*.rules\n.\n\n\nThe form of the workflow configuration file \nfor running rules that have no file targets is\nto specify the name of the rule:\n\n\n{\n    \"workflow_targets\" : <list of Snakemake rules>\n}\n\n\n\n\nor,\n\n\n{\n    \"workflow_targets\" : <list of output file targets>\n}\n\n\n\n\nExample Rule-Based Workflow Configuration File\n\n\nHere is a simple example of a workflow configuration file\nfor running a rule that has no file targets (pulling containers).\nThis rule specifies the workflow target by specifying the \nrule name:\n\n\ngoodies/test-conf.json\n:\n\n\n{\n    \"workflow_targets\" : [\"pull_biocontainers\"]\n}\n\n\n\n\nExample File-Based Workflow Configuration File\n\n\nHere is a simple example of a workflow configuration file\nthat runs rules based on filename targets (in this example,\nthe output of a trimming process):\n\n\ngoodies/test-conf.json\n:\n\n\n{\n    \"workflow_targets\" : [\"XXXXXX_trim5_1.fq.gz\",\n                          \"XXXXXX_trim5_2.fq.gz\"]\n}\n\n\n\n\nWorkflow Parameters File\n\n\nThe workflow parameters control various parameters\nthat are used by the programs and containers run\nby the Snakemake rules.\n\n\nEach workflow defines a set of default parameter values\nin a \n.settings\n file that lives next to the \nSnakefile\n\nand \n*.rule\n files.\n\n\nHowever, the user can set these parameter values in the \nworkflow parameters file, and these values set by the user \nwill override the default values.\n\n\nThe workflow parameters control how the workflow proceeds.\n\n\nThe workflow configuration controls the starting and ending points.\n\n\nThe form of the workflow parameters JSON file is:\n\n\n{\n    '<workflow-name>' : {\n        '<rule-name>' : {\n            '<param-name>' : <param-value>,\n            ...\n        }\n    }\n}\n\n\n\n\nMost parameters are listed under the name of the application\nthat uses it, but some parameters have more generic names because\nthey are common to multiple tasks.\n\n\nExample Workflow Parameters File\n\n\nThe following example parameters file adjusts the parameters for \nthe rule to update biocontainers.\n\n\nThis is a simple parameter file overriding a single parameter,\nthe version of sourmash being utilized.\n\n\nThis overrides the default biocontainers setting of \nsourmash version \n2.0.0a3--py36_0\n, set in \n\nrules/dahak/biocontainers.settings\n.\n\n\ngoodies/test-params.json\n:\n\n\n{\n    \"biocontainers\" : {\n        \"trimmomatic\" : {\n            \"use_local\" : false,\n            \"quayurl\" : \"quay.io/biocontainers/trimmomatic\",\n            \"version\" : \"0.36--5\"\n        }\n    }\n}\n\n\n\n\nRunning Taco\n\n\nTo run taco, pass it three arguments on the command line:\n\n\n\n\nName of the workflow\n\n\nRelative path to workflow configuration file\n\n\nRelative path to workflow parameters file\n\n\n\n\nBoth arguments should be the path to a JSON file\n(absolute or relative) and exclude the JSON suffix.\n\n\nRunning an Example\n\n\nTo run the two JSON files in the example above,\nwhich are contained in the \ngoodies/\n directory \nand which are the first steps in the taxonomic classification \nworkflow, we would run:\n\n\n$ ./taco taxonomic_classification goodies/test-conf.json goodies/test-params.json\n\n\n\n\n(The \n.json\n extension can be included or excluded.)\n\n\nTo do a dry run only, add the \n-n\n or \n--dry-run\n flag:\n\n\n$ ./taco -n taxonomic_classification goodies/test-conf goodies/test-params\n\n\n\n\nThis will run the \npull_biocontainers\n rule.\n\n\nListing Available Actions\n\n\nListing Workflows\n\n\nYou can list available taco workflows\nusing the \ntaco ls\n command with no \nadditional arguments:\n\n\n$ ./taco ls\n\n\n\n\nListing Workflow Rules\n\n\n(Not available)\n\n\nYou can list all of the rules available \nfor a given workflow by running \ntaco ls <workflow-name>\n.\nFor example:\n\n\n$ ./taco ls read_filtering",
            "title": "Quickstart"
        },
        {
            "location": "/Quickstart/#quick-start",
            "text": "The basic idea behind taco is to \npass it an action and modify basic\nbehavior with command line flags.  $ ./taco <action> --arguments  In this section we'll get started\nwith a simple workflow to illustrate\nhow to use taco.",
            "title": "Quick Start"
        },
        {
            "location": "/Quickstart/#workflow-repository",
            "text": "To create a new set of workflows, create a new repository.\nTo illustrate how to run taco, we will use a single workflow \nwith two steps.  Here is the repository layout for our simple example:  taco-simple/\n\n        rules/\n            my_simple_workflow/\n                Snakefile\n                step1.rule\n                workflow_A.settings\n\n        workflow-config/\n            configA.json\n            configB.json\n\n        workflow-params/\n            paramsA.json\n            paramsB.json",
            "title": "Workflow Repository"
        },
        {
            "location": "/Quickstart/#workflow-name",
            "text": "Each workflow is defined by a Snakefile that includes\na set of rules for each step of the workflow\nand a dictionary of parameter values.   Each workflow is in its own directory in the  rules/ \ndirectory of this repository and must have \na Snakefile.  The user should specify the workflow name as the first\nverb on the command line. For example:  $ taco read_filtering <workflow-config-file> <workflow-params-file>  will load the Snakefile in  rules/read_filtering/  \nand will make avaiable all read filtering rules.",
            "title": "Workflow Name"
        },
        {
            "location": "/Quickstart/#workflow-configuration-file",
            "text": "The workflow configuration file \ncontains a dictionary of configuration values.  The workflow configuration file\nmust specify a list of workflow targets\nassigned to the key  workflow_targets .  The value must be a valid Snakemake rule\ndefined in one of the rule files at  rules/<workflow-name>/*.rules .  The form of the workflow configuration file \nfor running rules that have no file targets is\nto specify the name of the rule:  {\n    \"workflow_targets\" : <list of Snakemake rules>\n}  or,  {\n    \"workflow_targets\" : <list of output file targets>\n}",
            "title": "Workflow Configuration File"
        },
        {
            "location": "/Quickstart/#example-rule-based-workflow-configuration-file",
            "text": "Here is a simple example of a workflow configuration file\nfor running a rule that has no file targets (pulling containers).\nThis rule specifies the workflow target by specifying the \nrule name:  goodies/test-conf.json :  {\n    \"workflow_targets\" : [\"pull_biocontainers\"]\n}",
            "title": "Example Rule-Based Workflow Configuration File"
        },
        {
            "location": "/Quickstart/#example-file-based-workflow-configuration-file",
            "text": "Here is a simple example of a workflow configuration file\nthat runs rules based on filename targets (in this example,\nthe output of a trimming process):  goodies/test-conf.json :  {\n    \"workflow_targets\" : [\"XXXXXX_trim5_1.fq.gz\",\n                          \"XXXXXX_trim5_2.fq.gz\"]\n}",
            "title": "Example File-Based Workflow Configuration File"
        },
        {
            "location": "/Quickstart/#workflow-parameters-file",
            "text": "The workflow parameters control various parameters\nthat are used by the programs and containers run\nby the Snakemake rules.  Each workflow defines a set of default parameter values\nin a  .settings  file that lives next to the  Snakefile \nand  *.rule  files.  However, the user can set these parameter values in the \nworkflow parameters file, and these values set by the user \nwill override the default values.  The workflow parameters control how the workflow proceeds.  The workflow configuration controls the starting and ending points.  The form of the workflow parameters JSON file is:  {\n    '<workflow-name>' : {\n        '<rule-name>' : {\n            '<param-name>' : <param-value>,\n            ...\n        }\n    }\n}  Most parameters are listed under the name of the application\nthat uses it, but some parameters have more generic names because\nthey are common to multiple tasks.",
            "title": "Workflow Parameters File"
        },
        {
            "location": "/Quickstart/#example-workflow-parameters-file",
            "text": "The following example parameters file adjusts the parameters for \nthe rule to update biocontainers.  This is a simple parameter file overriding a single parameter,\nthe version of sourmash being utilized.  This overrides the default biocontainers setting of \nsourmash version  2.0.0a3--py36_0 , set in  rules/dahak/biocontainers.settings .  goodies/test-params.json :  {\n    \"biocontainers\" : {\n        \"trimmomatic\" : {\n            \"use_local\" : false,\n            \"quayurl\" : \"quay.io/biocontainers/trimmomatic\",\n            \"version\" : \"0.36--5\"\n        }\n    }\n}",
            "title": "Example Workflow Parameters File"
        },
        {
            "location": "/Quickstart/#running-taco",
            "text": "To run taco, pass it three arguments on the command line:   Name of the workflow  Relative path to workflow configuration file  Relative path to workflow parameters file   Both arguments should be the path to a JSON file\n(absolute or relative) and exclude the JSON suffix.",
            "title": "Running Taco"
        },
        {
            "location": "/Quickstart/#running-an-example",
            "text": "To run the two JSON files in the example above,\nwhich are contained in the  goodies/  directory \nand which are the first steps in the taxonomic classification \nworkflow, we would run:  $ ./taco taxonomic_classification goodies/test-conf.json goodies/test-params.json  (The  .json  extension can be included or excluded.)  To do a dry run only, add the  -n  or  --dry-run  flag:  $ ./taco -n taxonomic_classification goodies/test-conf goodies/test-params  This will run the  pull_biocontainers  rule.",
            "title": "Running an Example"
        },
        {
            "location": "/Quickstart/#listing-available-actions",
            "text": "",
            "title": "Listing Available Actions"
        },
        {
            "location": "/Quickstart/#listing-workflows",
            "text": "You can list available taco workflows\nusing the  taco ls  command with no \nadditional arguments:  $ ./taco ls",
            "title": "Listing Workflows"
        },
        {
            "location": "/Quickstart/#listing-workflow-rules",
            "text": "(Not available)  You can list all of the rules available \nfor a given workflow by running  taco ls <workflow-name> .\nFor example:  $ ./taco ls read_filtering",
            "title": "Listing Workflow Rules"
        },
        {
            "location": "/SetupAWS/",
            "text": "AWS Worker Node Setup Walkthrough\n\n\nThis workflow covers getting set up with \nworker nodes, if you don't already have a\ncluster ready to go. \n\n\nIn this document we will:\n\n\n\n\nPrepare a beefy worker node to run a \n    dahak workflow by installing required software \n    from the dahak-yeti repository.\n\n\n\n\nWe will cover AWS in this document, although \ntaco can be used with various HPC and cloud \nplatforms.\n\n\nConsole\n\n\nLog into the AWS console and select EC2.  Select a region (\nus-west-2\n Oregon is good for spot instances).\n\n\nCreate Instance\n\n\nUse the big blue button to create a new EC2 instance.\n\n\nAmazon Machine Image\n\n\nSelect the stock Ubuntu 16.04 LTS image from the list of AMIs.\nThis has the AMI ID:\n\n\nami-4e79ed36\n\n\n\n\nNode Type\n\n\nThe hardware depends highly on the worfklow\n(more results soon from benchmarking of workflows),\nbut for all walkthroughs we utilize one of the following:\n\n\n\n\nm5.2xlarge\n (8 vCPUs, 32 GB RAM) \n\n\nm5.4xlarge\n (16 vCPUs, 64 GB RAM)\n\n\n\n\nConfiguring Instance Details\n\n\nOn the configure instance details:\n\n\n\n\n\n\nCheck \"Request Spot Instances\" box and set your desired price\n\n\n\n\nTypical price for 2xlarge is 14 cents per hour\n\n\nTypical price for 4xlarge is 28 cents per hour\n\n\n\n\n\n\n\n\nClick the Advanced Details bar at the bottom\n\n\n\n\n\n\nCopy the following into the user data text box:\n\n\n\n\n\n\n#!/bin/bash\nbash <( curl https://raw.githubusercontent.com/charlesreid1/dahak-yeti/master/cloud_init/cloud_init.sh )\n\n\n\n\n\n\n\n\nThe pipe-to-bash one-liner will run the cloud init script \n    in the \ndahak-yeti\n repo\n\n\n\n\nVolumes\n\n\nA 256 GB hard disk (EBS, the default) should be sufficient.\n\n\nLogging Into Instance\n\n\nWhen you create the node it should set up a private key\nto use to SSH into the worker node.\n\n\nThe init script will add a few minutes to the worker node's \nstartup time. It will run scripts, install files, set configurations,\nand run a lot of magic.\n\n\nOnce the startup step has completed, you can SSH to the\nworker node, and you will have the following installed:\n\n\n\n\npyenv\n\n\nminiconda3-4.30 installed with pyenv\n\n\nsnakemake installed with conda\n\n\nopinionated dotfiles (\n.bashrc\n, \n.bash_profile\n, \n.vimrc\n, etc.)\n\n\na colorful pink prompt",
            "title": "AWS Setup for Walkthroughs"
        },
        {
            "location": "/SetupAWS/#aws-worker-node-setup-walkthrough",
            "text": "This workflow covers getting set up with \nworker nodes, if you don't already have a\ncluster ready to go.   In this document we will:   Prepare a beefy worker node to run a \n    dahak workflow by installing required software \n    from the dahak-yeti repository.   We will cover AWS in this document, although \ntaco can be used with various HPC and cloud \nplatforms.",
            "title": "AWS Worker Node Setup Walkthrough"
        },
        {
            "location": "/SetupAWS/#console",
            "text": "Log into the AWS console and select EC2.  Select a region ( us-west-2  Oregon is good for spot instances).",
            "title": "Console"
        },
        {
            "location": "/SetupAWS/#create-instance",
            "text": "Use the big blue button to create a new EC2 instance.",
            "title": "Create Instance"
        },
        {
            "location": "/SetupAWS/#amazon-machine-image",
            "text": "Select the stock Ubuntu 16.04 LTS image from the list of AMIs.\nThis has the AMI ID:  ami-4e79ed36",
            "title": "Amazon Machine Image"
        },
        {
            "location": "/SetupAWS/#node-type",
            "text": "The hardware depends highly on the worfklow\n(more results soon from benchmarking of workflows),\nbut for all walkthroughs we utilize one of the following:   m5.2xlarge  (8 vCPUs, 32 GB RAM)   m5.4xlarge  (16 vCPUs, 64 GB RAM)",
            "title": "Node Type"
        },
        {
            "location": "/SetupAWS/#configuring-instance-details",
            "text": "On the configure instance details:    Check \"Request Spot Instances\" box and set your desired price   Typical price for 2xlarge is 14 cents per hour  Typical price for 4xlarge is 28 cents per hour     Click the Advanced Details bar at the bottom    Copy the following into the user data text box:    #!/bin/bash\nbash <( curl https://raw.githubusercontent.com/charlesreid1/dahak-yeti/master/cloud_init/cloud_init.sh )    The pipe-to-bash one-liner will run the cloud init script \n    in the  dahak-yeti  repo",
            "title": "Configuring Instance Details"
        },
        {
            "location": "/SetupAWS/#volumes",
            "text": "A 256 GB hard disk (EBS, the default) should be sufficient.",
            "title": "Volumes"
        },
        {
            "location": "/SetupAWS/#logging-into-instance",
            "text": "When you create the node it should set up a private key\nto use to SSH into the worker node.  The init script will add a few minutes to the worker node's \nstartup time. It will run scripts, install files, set configurations,\nand run a lot of magic.  Once the startup step has completed, you can SSH to the\nworker node, and you will have the following installed:   pyenv  miniconda3-4.30 installed with pyenv  snakemake installed with conda  opinionated dotfiles ( .bashrc ,  .bash_profile ,  .vimrc , etc.)  a colorful pink prompt",
            "title": "Logging Into Instance"
        },
        {
            "location": "/SnakemakeRules/",
            "text": "Snakemake Rules\n\n\nA brief summary of how Snakemake rules are implemented and organized.\n\n\nWe follow the conventions set in the \nsnakemake-rules\n\nrepository, and groups all rules into \n.rule\n files and all parameters into\n\n.settings\n files.\n\n\nWorkflows\n\n\nThe workflows define a set of tasks.\nEach workflow is a sub-directory in the \nrules/\n \ndirectory.\n\n\nRule Files\n\n\nRule files define how steps in the workflow proceed.\nThey preprocess configuration variables.\nRules are defined in \n.rule\n files\nin the workflow directory.\n\n\nDefault Parameter Files\n\n\nThe default parameter dictionary is contained\nin a \n.settings\n file in the workflow directory.\nThese will \nNOT\n overwrite any parameters\nthat have already been set by the user in their\nJSON parameter file.\n\n\nValidation of Parameters\n\n\nBecause we don't know what workflow is being run\nwhen we process the parameters, we can't do much\nparameter validation.\n\n\nCurrently, we do not throw an exception \nwhen a parameter is undefined by the user,\nwe either silently use the default value \n(default behavior) or we silently set to \nan empty string (using \n--clean\n or \n-c\n\nflag).",
            "title": "Snakemake Rules"
        },
        {
            "location": "/SnakemakeRules/#snakemake-rules",
            "text": "A brief summary of how Snakemake rules are implemented and organized.  We follow the conventions set in the  snakemake-rules \nrepository, and groups all rules into  .rule  files and all parameters into .settings  files.",
            "title": "Snakemake Rules"
        },
        {
            "location": "/SnakemakeRules/#workflows",
            "text": "The workflows define a set of tasks.\nEach workflow is a sub-directory in the  rules/  \ndirectory.",
            "title": "Workflows"
        },
        {
            "location": "/SnakemakeRules/#rule-files",
            "text": "Rule files define how steps in the workflow proceed.\nThey preprocess configuration variables.\nRules are defined in  .rule  files\nin the workflow directory.",
            "title": "Rule Files"
        },
        {
            "location": "/SnakemakeRules/#default-parameter-files",
            "text": "The default parameter dictionary is contained\nin a  .settings  file in the workflow directory.\nThese will  NOT  overwrite any parameters\nthat have already been set by the user in their\nJSON parameter file.",
            "title": "Default Parameter Files"
        },
        {
            "location": "/SnakemakeRules/#validation-of-parameters",
            "text": "Because we don't know what workflow is being run\nwhen we process the parameters, we can't do much\nparameter validation.  Currently, we do not throw an exception \nwhen a parameter is undefined by the user,\nwe either silently use the default value \n(default behavior) or we silently set to \nan empty string (using  --clean  or  -c \nflag).",
            "title": "Validation of Parameters"
        },
        {
            "location": "/Workflows/",
            "text": "Adding New Taco Workflows\n\n\nIf you wish to extend or modify taco by \nadding a new workflow, you can do so,\nbut it's important to plan it out first!\n\n\nEach Snakemake workflow is composed of \na set of loosely-interconnected rules \nsharing parameters and handling pieces \nof the whole workflow.\n\n\nThe workflow lives in a directory \nin \nrules\n that is named after the \nworkflow. For example, to add a new\nworkflow called \nfoobar\n:\n\n\nrules/\n    foobar/\n    taxonomic_classification/\n    read_filtering/\n    functional_annotation/\n    ...\n\n\n\n\nEach rule or group of rules lives in a \n.rule file. All rule files have access\nto the entire configuration dictionary,\nwhich stores all workflow parameters.\n\n\nSuppose our new workflow \nfoobar\n had \nthree distinct steps: \nbuz\n, \nfuz\n, and \nwuz\n.\nThen we would create a rule file for each,\nand the \nfoobar/\n rules directory structure\nwould be:\n\n\nrules/\n    foobar/\n        buz.rule\n        fuz.rule\n        wuz.rule\n        foobar.settings\n\n\n\n\nAdditionally, we need to set parameters for the workflow.\n\n\nEach workflow has access to the \"master\" Snakemake\nparameters dictionary. \n\n\nIn addition, each workflow defines its own parameters dictionary\nto store parameters specific to that workflow.\n\n\nThis parameters dictionary is stored under a key that is the name \nof the workflow.\n\n\nFor example, if each of our three workflow steps took \nparameters, here is how we would organize the \nworkflow's default parameters dictionary:\n\n\n{\n    ...\n\n    'foobar' : {\n\n        'buz' : {\n            'buz_param_1' : 1,\n            'buz_param_2' : 'alpha'\n        },\n\n        'fuz' : {\n            'fuz_param_1' : 50,\n            'fuz_param_2' : 51,\n            'fuz_param_3' : False\n        },\n\n        'wuz' : {\n            'wuz_param_1' : 9.99,\n            'wuz_param_2' : 9.98,\n            'wuz_param_3' : 9.97,\n            'wuz_param_4' : 9.96\n        }\n    }\n    ...\n}\n\n\n\n\nThe \nfoobar.settings\n file must set this \nin a way that will not overwrite defaults;\nhence this business:\n\n\nfrom snakemake.utils import update_config\n\nif(not config['clean']):\n\n    # Note: don't include http:// or https://\n    config_default = { \n                        ... \n                     }\n\n    update_config(config_default, config)\n    config = config_default",
            "title": "Creating Workflows"
        },
        {
            "location": "/Workflows/#adding-new-taco-workflows",
            "text": "If you wish to extend or modify taco by \nadding a new workflow, you can do so,\nbut it's important to plan it out first!  Each Snakemake workflow is composed of \na set of loosely-interconnected rules \nsharing parameters and handling pieces \nof the whole workflow.  The workflow lives in a directory \nin  rules  that is named after the \nworkflow. For example, to add a new\nworkflow called  foobar :  rules/\n    foobar/\n    taxonomic_classification/\n    read_filtering/\n    functional_annotation/\n    ...  Each rule or group of rules lives in a \n.rule file. All rule files have access\nto the entire configuration dictionary,\nwhich stores all workflow parameters.  Suppose our new workflow  foobar  had \nthree distinct steps:  buz ,  fuz , and  wuz .\nThen we would create a rule file for each,\nand the  foobar/  rules directory structure\nwould be:  rules/\n    foobar/\n        buz.rule\n        fuz.rule\n        wuz.rule\n        foobar.settings  Additionally, we need to set parameters for the workflow.  Each workflow has access to the \"master\" Snakemake\nparameters dictionary.   In addition, each workflow defines its own parameters dictionary\nto store parameters specific to that workflow.  This parameters dictionary is stored under a key that is the name \nof the workflow.  For example, if each of our three workflow steps took \nparameters, here is how we would organize the \nworkflow's default parameters dictionary:  {\n    ...\n\n    'foobar' : {\n\n        'buz' : {\n            'buz_param_1' : 1,\n            'buz_param_2' : 'alpha'\n        },\n\n        'fuz' : {\n            'fuz_param_1' : 50,\n            'fuz_param_2' : 51,\n            'fuz_param_3' : False\n        },\n\n        'wuz' : {\n            'wuz_param_1' : 9.99,\n            'wuz_param_2' : 9.98,\n            'wuz_param_3' : 9.97,\n            'wuz_param_4' : 9.96\n        }\n    }\n    ...\n}  The  foobar.settings  file must set this \nin a way that will not overwrite defaults;\nhence this business:  from snakemake.utils import update_config\n\nif(not config['clean']):\n\n    # Note: don't include http:// or https://\n    config_default = { \n                        ... \n                     }\n\n    update_config(config_default, config)\n    config = config_default",
            "title": "Adding New Taco Workflows"
        }
    ]
}
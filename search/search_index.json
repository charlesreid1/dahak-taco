{
    "docs": [
        {
            "location": "/",
            "text": "dahak-taco documentation\n\n\ndahak-taco is an experimental command-line interface\nfor running dahak workflows using Snakemake.\n\n\n(insert icholy/ttygif here.)\n\n\nTo get started with taco,\nand run your first workflow task,\nsee the Getting Started Section below.\n\n\nIf you're hungry for more dahak workflows,\nskip to the Walkthroughs Section below.\n\n\nIf you are already up and running \nwith taco and are looking for \ninformation about the rules and their \nparameters, check out the \nAPI Section below.\n\n\nIf you are extending taco by adding new\nrules, workflows, or documentation, see \nthe :ref:\nfor-developers-label\n section.\n\n\nGetting Started\n\n\ntaco is a command line utility that wraps\nSnakemake rules to run complex workflows.\n\n\nThese sections will cover what taco is\nand get you up and running with your \nfirst taco workflow.\n\n\nIntro\n\n\nQuickstart\n\n\nWorkflow Walkthroughs\n\n\ntaco is a lightweight wrapper around Snakemake tasks.\nKind of like a corn tortilla.\n\n\nThe Snakemake rules and parameters are organized \nby workflow in the \nrules/\n directory \nof this repository.\n\n\nThe walkthroughs below show examples\nof how to run each workflow.\n\n\nAWS Setup\n\n\nRead Filtering Walkthrough\n\n\nWorkflow Parameters\n\n\nEach workflow takes a parameters dictionary.\nThis section details the structure and keys\nthat are defined and used in the parameter \ndictionary.\n\n\nRead Filtering Workflow Parameters\n\n\nFor Developers\n\n\nThe sections below explain how taco works,\nso you know how to modify taco to suit your needs.\n\n\nThere are also sections for adding or modifying\nSnakemake rules, and for adding new workflows.\n\n\ntaco provides a set of workflows with default \nparameters that should work for many use cases.\n\n\nHowever, to extend taco, or just understand\nwhat it is doing, take a look at the innards of taco:\n\n\nHow It Works\n\n\nSnakemake Rules\n\n\nWorkflows\n\n\nDocumentation",
            "title": "Index"
        },
        {
            "location": "/#dahak-taco-documentation",
            "text": "dahak-taco is an experimental command-line interface\nfor running dahak workflows using Snakemake.  (insert icholy/ttygif here.)  To get started with taco,\nand run your first workflow task,\nsee the Getting Started Section below.  If you're hungry for more dahak workflows,\nskip to the Walkthroughs Section below.  If you are already up and running \nwith taco and are looking for \ninformation about the rules and their \nparameters, check out the \nAPI Section below.  If you are extending taco by adding new\nrules, workflows, or documentation, see \nthe :ref: for-developers-label  section.",
            "title": "dahak-taco documentation"
        },
        {
            "location": "/#getting-started",
            "text": "taco is a command line utility that wraps\nSnakemake rules to run complex workflows.  These sections will cover what taco is\nand get you up and running with your \nfirst taco workflow.  Intro  Quickstart",
            "title": "Getting Started"
        },
        {
            "location": "/#workflow-walkthroughs",
            "text": "taco is a lightweight wrapper around Snakemake tasks.\nKind of like a corn tortilla.  The Snakemake rules and parameters are organized \nby workflow in the  rules/  directory \nof this repository.  The walkthroughs below show examples\nof how to run each workflow.  AWS Setup  Read Filtering Walkthrough",
            "title": "Workflow Walkthroughs"
        },
        {
            "location": "/#workflow-parameters",
            "text": "Each workflow takes a parameters dictionary.\nThis section details the structure and keys\nthat are defined and used in the parameter \ndictionary.  Read Filtering Workflow Parameters",
            "title": "Workflow Parameters"
        },
        {
            "location": "/#for-developers",
            "text": "The sections below explain how taco works,\nso you know how to modify taco to suit your needs.  There are also sections for adding or modifying\nSnakemake rules, and for adding new workflows.  taco provides a set of workflows with default \nparameters that should work for many use cases.  However, to extend taco, or just understand\nwhat it is doing, take a look at the innards of taco:  How It Works  Snakemake Rules  Workflows  Documentation",
            "title": "For Developers"
        },
        {
            "location": "/intro/Intro/",
            "text": "Introduction: dahak-taco\n\n\ndahak-taco is an experimental command line interface for running dahak workflows.\n\n\ntaco is hosted in Github at \n\nhttps://github.com/dahak-metagenomics/dahak-taco\n.\n\n\ntaco documentation is hosted by Github at \n\nhttps://dahak-metagenomics.github.io/dahak-taco\n\n\nInstallation\n\n\nFor installation instructions, refer to the \n\nINSTALL.md\n.\nfile.\n\n\nHow To Use Taco\n\n\nThe file \ntaco\n contains the command line tool,\nso run it directly:\n\n\n$ ./taco <arguments>\n\n\n\n\nThis will provide helpful information at the command line. \n\n\nKeep reading for a description of what \ninformation taco requires to run.",
            "title": "Overview of Taco"
        },
        {
            "location": "/intro/Intro/#introduction-dahak-taco",
            "text": "dahak-taco is an experimental command line interface for running dahak workflows.  taco is hosted in Github at  https://github.com/dahak-metagenomics/dahak-taco .  taco documentation is hosted by Github at  https://dahak-metagenomics.github.io/dahak-taco",
            "title": "Introduction: dahak-taco"
        },
        {
            "location": "/intro/Intro/#installation",
            "text": "For installation instructions, refer to the  INSTALL.md .\nfile.",
            "title": "Installation"
        },
        {
            "location": "/intro/Intro/#how-to-use-taco",
            "text": "The file  taco  contains the command line tool,\nso run it directly:  $ ./taco <arguments>  This will provide helpful information at the command line.   Keep reading for a description of what \ninformation taco requires to run.",
            "title": "How To Use Taco"
        },
        {
            "location": "/intro/Installing/",
            "text": "Installing\n\n\ntaco is an executable script, so there is nothing to install\nfor taco specifically - you can just download and run taco.\n(However, taco requires other software be installed; see below.)\n\n\nRequirements\n\n\n\n\nPython 3\n\n\nConda (Miniconda recommended)\n\n\nSnakemake\n\n\nSingularity\n\n\n\n\nOptional\n\n\n\n\nDocker\n\n\n\n\nResolving Requirements\n\n\nInstall Scripts\n\n\nScripts to install the above packages can be found\nin the \nscripts/\n \ndirectory of dahak-taco.\n\n\nTesting Requirements are Installed\n\n\nTo check that you are using Python 3, run:\n\n\n$ /usr/bin/env python --version\n\n\n\n\nTo test that Snakemake can be imported from Python, run:\n\n\n$ /usr/bin/env python -c 'import snakemake'\n\n\n\n\nGetting Taco\n\n\nTo get taco, clone a local copy using git:\n\n\n$ git clone https://github.com/dahak-metagenomics/dahak-taco.git \n$ cd dahak-taco/\n$ ./taco --help",
            "title": "Installing Taco"
        },
        {
            "location": "/intro/Installing/#installing",
            "text": "taco is an executable script, so there is nothing to install\nfor taco specifically - you can just download and run taco.\n(However, taco requires other software be installed; see below.)",
            "title": "Installing"
        },
        {
            "location": "/intro/Installing/#requirements",
            "text": "Python 3  Conda (Miniconda recommended)  Snakemake  Singularity",
            "title": "Requirements"
        },
        {
            "location": "/intro/Installing/#optional",
            "text": "Docker",
            "title": "Optional"
        },
        {
            "location": "/intro/Installing/#resolving-requirements",
            "text": "",
            "title": "Resolving Requirements"
        },
        {
            "location": "/intro/Installing/#install-scripts",
            "text": "Scripts to install the above packages can be found\nin the  scripts/  \ndirectory of dahak-taco.",
            "title": "Install Scripts"
        },
        {
            "location": "/intro/Installing/#testing-requirements-are-installed",
            "text": "To check that you are using Python 3, run:  $ /usr/bin/env python --version  To test that Snakemake can be imported from Python, run:  $ /usr/bin/env python -c 'import snakemake'",
            "title": "Testing Requirements are Installed"
        },
        {
            "location": "/intro/Installing/#getting-taco",
            "text": "To get taco, clone a local copy using git:  $ git clone https://github.com/dahak-metagenomics/dahak-taco.git \n$ cd dahak-taco/\n$ ./taco --help",
            "title": "Getting Taco"
        },
        {
            "location": "/intro/Quickstart/",
            "text": "Quick Start\n\n\nRun the taco command line tool like this:\n\n\n$ ./taco <arguments>\n\n\n\n\ndahak-taco requires three inputs from the user:\n\n\n\n\n\n\nWorkflow name:\n specifies which workflow to run \n    (and which set of Snakemake rules will be defined).\n\n\n\n\n\n\nWorkflow configuration file:\n specifies the workflow rules\n    to run and the names of the target input and output files.\n    These rules are defined in \nrules/dahak/*.rules\n.\n\n\n\n\n\n\nWorkflow parameters file (optional):\n specifies values for \n    workflow parameters. Parameters set in this\n    parameters file will override parameters set\n    in the \nrules/dahak/*.settings\n files.\n\n\n\n\n\n\nTypes: \n\n\nWorkflow name\n is a string given on the command line.\n\n\nWorkflow configuration file\n and \nWorkflow parameters file\n are both JSON files.\n\n\nThe form is:\n\n\n$ ./taco <workflow-name> <workflow-config-file> <workflow-params-file>\n\n\n\n\nCheck the \ngoodies/\n \ndirectory for examples.\n\n\nWorkflow Name\n\n\nEach workflow is defined by a Snakefile that includes\na set of rules for each step of the workflow\nand a dictionary of parameter values. \n\n\nEach workflow is in its own directory in the \n\nrules/\n\ndirectory of this repository and must have \na Snakefile.\n\n\nThe user should specify the workflow name as the first\nverb on the command line. For example:\n\n\n$ taco read_filtering <workflow-config-file> <workflow-params-file>\n\n\n\n\nwill load the Snakefile in \nrules/read_filtering/\n \nand will make avaiable all read filtering rules.\n\n\nWorkflow Configuration File\n\n\nThe workflow configuration file \ncontains a dictionary of configuration values.\n\n\nThe workflow configuration file\nmust specify a list of workflow targets\nassigned to the key \nworkflow_targets\n.\n\n\nThe value must be a valid Snakemake rule\ndefined in one of the rule files at \nrules/<workflow-name>/*.rules\n.\n\n\nThe form of the workflow configuration file \nfor running rules that have no file targets is\nto specify the name of the rule:\n\n\n{\n    \"workflow_targets\" : <list of Snakemake rules>\n}\n\n\n\n\nor,\n\n\n{\n    \"workflow_targets\" : <list of output file targets>\n}\n\n\n\n\nExample Rule-Based Workflow Configuration File\n\n\nHere is a simple example of a workflow configuration file\nfor running a rule that has no file targets (pulling containers).\nThis rule specifies the workflow target by specifying the \nrule name:\n\n\ngoodies/test-conf.json\n:\n\n\n{\n    \"workflow_targets\" : [\"pull_biocontainers\"]\n}\n\n\n\n\nExample File-Based Workflow Configuration File\n\n\nHere is a simple example of a workflow configuration file\nthat runs rules based on filename targets (in this example,\nthe output of a trimming process):\n\n\ngoodies/test-conf.json\n:\n\n\n{\n    \"workflow_targets\" : [\"XXXXXX_trim5_1.fq.gz\",\n                          \"XXXXXX_trim5_2.fq.gz\"]\n}\n\n\n\n\nWorkflow Parameters File\n\n\nThe workflow parameters control various parameters\nthat are used by the programs and containers run\nby the Snakemake rules.\n\n\nEach workflow defines a set of default parameter values\nin a \n.settings\n file that lives next to the \nSnakefile\n\nand \n*.rule\n files.\n\n\nHowever, the user can set these parameter values in the \nworkflow parameters file, and these values set by the user \nwill override the default values.\n\n\nThe workflow parameters control how the workflow proceeds.\n\n\nThe workflow configuration controls the starting and ending points.\n\n\nThe form of the workflow parameters JSON file is:\n\n\n{\n    '<workflow-name>' : {\n        '<rule-name>' : {\n            '<param-name>' : <param-value>,\n            ...\n        }\n    }\n}\n\n\n\n\nMost parameters are listed under the name of the application\nthat uses it, but some parameters have more generic names because\nthey are common to multiple tasks.\n\n\nExample Workflow Parameters File\n\n\nThe following example parameters file adjusts the parameters for \nthe rule to update biocontainers.\n\n\nThis is a simple parameter file overriding a single parameter,\nthe version of sourmash being utilized.\n\n\nThis overrides the default biocontainers setting of \nsourmash version \n2.0.0a3--py36_0\n, set in \n\nrules/dahak/biocontainers.settings\n.\n\n\ngoodies/test-params.json\n:\n\n\n{\n    \"biocontainers\" : {\n        \"trimmomatic\" : {\n            \"use_local\" : false,\n            \"quayurl\" : \"quay.io/biocontainers/trimmomatic\",\n            \"version\" : \"0.36--5\"\n        }\n    }\n}\n\n\n\n\nRunning Taco\n\n\nTo run taco, pass it three arguments on the command line:\n\n\n\n\nName of the workflow\n\n\nRelative path to workflow configuration file\n\n\nRelative path to workflow parameters file\n\n\n\n\nBoth arguments should be the path to a JSON file\n(absolute or relative) and exclude the JSON suffix.\n\n\nRunning an Example\n\n\nTo run the two JSON files in the example above,\nwhich are contained in the \ngoodies/\n directory \nand which are the first steps in the taxonomic classification \nworkflow, we would run:\n\n\n$ ./taco taxonomic_classification goodies/test-conf.json goodies/test-params.json\n\n\n\n\n(The \n.json\n extension can be included or excluded.)\n\n\nTo do a dry run only, add the \n-n\n or \n--dry-run\n flag:\n\n\n$ ./taco -n taxonomic_classification goodies/test-conf goodies/test-params\n\n\n\n\nThis will run the \npull_biocontainers\n rule.\n\n\nListing Available Actions\n\n\nListing Workflows\n\n\nYou can list available taco workflows\nusing the \ntaco ls\n command with no \nadditional arguments:\n\n\n$ ./taco ls\n\n\n\n\nListing Workflow Rules\n\n\n(Not available)\n\n\nYou can list all of the rules available \nfor a given workflow by running \ntaco ls <workflow-name>\n.\nFor example:\n\n\n$ ./taco ls read_filtering",
            "title": "Quickstart"
        },
        {
            "location": "/intro/Quickstart/#quick-start",
            "text": "Run the taco command line tool like this:  $ ./taco <arguments>  dahak-taco requires three inputs from the user:    Workflow name:  specifies which workflow to run \n    (and which set of Snakemake rules will be defined).    Workflow configuration file:  specifies the workflow rules\n    to run and the names of the target input and output files.\n    These rules are defined in  rules/dahak/*.rules .    Workflow parameters file (optional):  specifies values for \n    workflow parameters. Parameters set in this\n    parameters file will override parameters set\n    in the  rules/dahak/*.settings  files.    Types:   Workflow name  is a string given on the command line.  Workflow configuration file  and  Workflow parameters file  are both JSON files.  The form is:  $ ./taco <workflow-name> <workflow-config-file> <workflow-params-file>  Check the  goodies/  \ndirectory for examples.",
            "title": "Quick Start"
        },
        {
            "location": "/intro/Quickstart/#workflow-name",
            "text": "Each workflow is defined by a Snakefile that includes\na set of rules for each step of the workflow\nand a dictionary of parameter values.   Each workflow is in its own directory in the  rules/ \ndirectory of this repository and must have \na Snakefile.  The user should specify the workflow name as the first\nverb on the command line. For example:  $ taco read_filtering <workflow-config-file> <workflow-params-file>  will load the Snakefile in  rules/read_filtering/  \nand will make avaiable all read filtering rules.",
            "title": "Workflow Name"
        },
        {
            "location": "/intro/Quickstart/#workflow-configuration-file",
            "text": "The workflow configuration file \ncontains a dictionary of configuration values.  The workflow configuration file\nmust specify a list of workflow targets\nassigned to the key  workflow_targets .  The value must be a valid Snakemake rule\ndefined in one of the rule files at  rules/<workflow-name>/*.rules .  The form of the workflow configuration file \nfor running rules that have no file targets is\nto specify the name of the rule:  {\n    \"workflow_targets\" : <list of Snakemake rules>\n}  or,  {\n    \"workflow_targets\" : <list of output file targets>\n}",
            "title": "Workflow Configuration File"
        },
        {
            "location": "/intro/Quickstart/#example-rule-based-workflow-configuration-file",
            "text": "Here is a simple example of a workflow configuration file\nfor running a rule that has no file targets (pulling containers).\nThis rule specifies the workflow target by specifying the \nrule name:  goodies/test-conf.json :  {\n    \"workflow_targets\" : [\"pull_biocontainers\"]\n}",
            "title": "Example Rule-Based Workflow Configuration File"
        },
        {
            "location": "/intro/Quickstart/#example-file-based-workflow-configuration-file",
            "text": "Here is a simple example of a workflow configuration file\nthat runs rules based on filename targets (in this example,\nthe output of a trimming process):  goodies/test-conf.json :  {\n    \"workflow_targets\" : [\"XXXXXX_trim5_1.fq.gz\",\n                          \"XXXXXX_trim5_2.fq.gz\"]\n}",
            "title": "Example File-Based Workflow Configuration File"
        },
        {
            "location": "/intro/Quickstart/#workflow-parameters-file",
            "text": "The workflow parameters control various parameters\nthat are used by the programs and containers run\nby the Snakemake rules.  Each workflow defines a set of default parameter values\nin a  .settings  file that lives next to the  Snakefile \nand  *.rule  files.  However, the user can set these parameter values in the \nworkflow parameters file, and these values set by the user \nwill override the default values.  The workflow parameters control how the workflow proceeds.  The workflow configuration controls the starting and ending points.  The form of the workflow parameters JSON file is:  {\n    '<workflow-name>' : {\n        '<rule-name>' : {\n            '<param-name>' : <param-value>,\n            ...\n        }\n    }\n}  Most parameters are listed under the name of the application\nthat uses it, but some parameters have more generic names because\nthey are common to multiple tasks.",
            "title": "Workflow Parameters File"
        },
        {
            "location": "/intro/Quickstart/#example-workflow-parameters-file",
            "text": "The following example parameters file adjusts the parameters for \nthe rule to update biocontainers.  This is a simple parameter file overriding a single parameter,\nthe version of sourmash being utilized.  This overrides the default biocontainers setting of \nsourmash version  2.0.0a3--py36_0 , set in  rules/dahak/biocontainers.settings .  goodies/test-params.json :  {\n    \"biocontainers\" : {\n        \"trimmomatic\" : {\n            \"use_local\" : false,\n            \"quayurl\" : \"quay.io/biocontainers/trimmomatic\",\n            \"version\" : \"0.36--5\"\n        }\n    }\n}",
            "title": "Example Workflow Parameters File"
        },
        {
            "location": "/intro/Quickstart/#running-taco",
            "text": "To run taco, pass it three arguments on the command line:   Name of the workflow  Relative path to workflow configuration file  Relative path to workflow parameters file   Both arguments should be the path to a JSON file\n(absolute or relative) and exclude the JSON suffix.",
            "title": "Running Taco"
        },
        {
            "location": "/intro/Quickstart/#running-an-example",
            "text": "To run the two JSON files in the example above,\nwhich are contained in the  goodies/  directory \nand which are the first steps in the taxonomic classification \nworkflow, we would run:  $ ./taco taxonomic_classification goodies/test-conf.json goodies/test-params.json  (The  .json  extension can be included or excluded.)  To do a dry run only, add the  -n  or  --dry-run  flag:  $ ./taco -n taxonomic_classification goodies/test-conf goodies/test-params  This will run the  pull_biocontainers  rule.",
            "title": "Running an Example"
        },
        {
            "location": "/intro/Quickstart/#listing-available-actions",
            "text": "",
            "title": "Listing Available Actions"
        },
        {
            "location": "/intro/Quickstart/#listing-workflows",
            "text": "You can list available taco workflows\nusing the  taco ls  command with no \nadditional arguments:  $ ./taco ls",
            "title": "Listing Workflows"
        },
        {
            "location": "/intro/Quickstart/#listing-workflow-rules",
            "text": "(Not available)  You can list all of the rules available \nfor a given workflow by running  taco ls <workflow-name> .\nFor example:  $ ./taco ls read_filtering",
            "title": "Listing Workflow Rules"
        },
        {
            "location": "/walkthrus/AWS_Setup/",
            "text": "AWS Worker Node Setup Walkthrough\n\n\nThis workflow covers getting set up with \nworker nodes, if you don't already have a\ncluster ready to go. \n\n\nIn this document we will:\n\n\n\n\nPrepare a beefy worker node to run a \n    dahak workflow by installing required software \n    from the dahak-yeti repository.\n\n\n\n\nWe will cover AWS in this document, although \ndahak-taco can be used with various HPC and \ncloud platforms.\n\n\nConsole\n\n\nLog into the AWS console and select EC2.\n\n\nSelect a region (\nus-west-2\n Oregon is good for spot instances).\n\n\nCreate Instance\n\n\nUse the big blue button to create a new EC2 instance.\n\n\nAmazon Machine Image\n\n\nSelect the stock Ubuntu 16.04 LTS image \nfrom the list of AMIs.\nThis has the AMI ID:\n\n\nami-4e79ed36\n\n\n\n\nNode Type\n\n\nThe hardware depends highly on the worfklow\n(more results soon from benchmarking of workflows),\nbut for all walkthroughs we utilize one of the following:\n\n\n\n\nm5.2xlarge\n (8 vCPUs, 32 GB RAM) \n\n\nm5.4xlarge\n (16 vCPUs, 64 GB RAM)\n\n\n\n\nSelect \nm5.2xlarge\n for the read filtering walkthrough.\n\n\nConfiguring Instance Details\n\n\nOn the configure instance details:\n\n\n\n\n\n\nCheck \"Request Spot Instances\" box and set your desired price\n\n\n\n\nTypical price for 2xlarge is 14 cents per hour\n\n\nTypical price for 4xlarge is 28 cents per hour\n\n\n\n\n\n\n\n\nClick the Advanced Details bar at the bottom\n\n\n\n\n\n\nCopy the following into the user data text box:\n\n\n\n\n\n\n#!/bin/bash\nbash <( curl https://raw.githubusercontent.com/charlesreid1/dahak-yeti/master/cloud_init/cloud_init.sh )\n\n\n\n\n\n\n\n\nThe pipe-to-bash one-liner will run the cloud init script \n    in the \ndahak-yeti\n repo\n\n\n\n\nVolumes\n\n\nA 256 GB hard disk (EBS, the default) should be sufficient.\n\n\nLogging Into Instance\n\n\nWhen you create the node it should set up a private key\nto use to SSH into the worker node.\n\n\nThe init script will add a few minutes to the worker node's \nstartup time. It will run scripts, install files, set configurations,\nand run a lot of magic.\n\n\nOnce the startup step has completed, you can SSH to the\nworker node, and you will have the following installed:\n\n\n\n\npyenv\n\n\nminiconda3-4.30 installed with pyenv\n\n\nsnakemake installed with conda\n\n\nopinionated dotfiles (\n.bashrc\n, \n.bash_profile\n, \n.vimrc\n, etc.)\n\n\na colorful pink prompt",
            "title": "AWS Setup"
        },
        {
            "location": "/walkthrus/AWS_Setup/#aws-worker-node-setup-walkthrough",
            "text": "This workflow covers getting set up with \nworker nodes, if you don't already have a\ncluster ready to go.   In this document we will:   Prepare a beefy worker node to run a \n    dahak workflow by installing required software \n    from the dahak-yeti repository.   We will cover AWS in this document, although \ndahak-taco can be used with various HPC and \ncloud platforms.",
            "title": "AWS Worker Node Setup Walkthrough"
        },
        {
            "location": "/walkthrus/AWS_Setup/#console",
            "text": "Log into the AWS console and select EC2.  Select a region ( us-west-2  Oregon is good for spot instances).",
            "title": "Console"
        },
        {
            "location": "/walkthrus/AWS_Setup/#create-instance",
            "text": "Use the big blue button to create a new EC2 instance.",
            "title": "Create Instance"
        },
        {
            "location": "/walkthrus/AWS_Setup/#amazon-machine-image",
            "text": "Select the stock Ubuntu 16.04 LTS image \nfrom the list of AMIs.\nThis has the AMI ID:  ami-4e79ed36",
            "title": "Amazon Machine Image"
        },
        {
            "location": "/walkthrus/AWS_Setup/#node-type",
            "text": "The hardware depends highly on the worfklow\n(more results soon from benchmarking of workflows),\nbut for all walkthroughs we utilize one of the following:   m5.2xlarge  (8 vCPUs, 32 GB RAM)   m5.4xlarge  (16 vCPUs, 64 GB RAM)   Select  m5.2xlarge  for the read filtering walkthrough.",
            "title": "Node Type"
        },
        {
            "location": "/walkthrus/AWS_Setup/#configuring-instance-details",
            "text": "On the configure instance details:    Check \"Request Spot Instances\" box and set your desired price   Typical price for 2xlarge is 14 cents per hour  Typical price for 4xlarge is 28 cents per hour     Click the Advanced Details bar at the bottom    Copy the following into the user data text box:    #!/bin/bash\nbash <( curl https://raw.githubusercontent.com/charlesreid1/dahak-yeti/master/cloud_init/cloud_init.sh )    The pipe-to-bash one-liner will run the cloud init script \n    in the  dahak-yeti  repo",
            "title": "Configuring Instance Details"
        },
        {
            "location": "/walkthrus/AWS_Setup/#volumes",
            "text": "A 256 GB hard disk (EBS, the default) should be sufficient.",
            "title": "Volumes"
        },
        {
            "location": "/walkthrus/AWS_Setup/#logging-into-instance",
            "text": "When you create the node it should set up a private key\nto use to SSH into the worker node.  The init script will add a few minutes to the worker node's \nstartup time. It will run scripts, install files, set configurations,\nand run a lot of magic.  Once the startup step has completed, you can SSH to the\nworker node, and you will have the following installed:   pyenv  miniconda3-4.30 installed with pyenv  snakemake installed with conda  opinionated dotfiles ( .bashrc ,  .bash_profile ,  .vimrc , etc.)  a colorful pink prompt",
            "title": "Logging Into Instance"
        },
        {
            "location": "/walkthrus/ReadFiltWalkthru/",
            "text": "Read Filtering Walkthrough\n\n\nThis walkthrough covers a read filtering workflow. \n\n\nIn this document we will:\n\n\n\n\n\n\nDownload raw sequence data from OSF data store (wget).\n\n\n\n\n\n\nAssess the quality of the pre-filtered reads (fastqc)\n\n\n\n\n\n\nTrim and filter the reads based on a quality threshold (trimmomatic)\n\n\n\n\n\n\nAssess the quality of the post-filtered reads (fastqc)\n\n\n\n\n\n\nInterleave post-filtered reads (khmer) \n\n\n\n\n\n\nThis workflow covers the use of taco to run the workflow.\n\n\nSee the \nwalkthroughs dir of the dahak repo\n\nfor the original shell-based walkthrough.\n\n\nSetup\n\n\nThis node assumes you have the following software:\n\n\n\n\nPython\n\n\nConda\n\n\nSnakemake\n\n\nSingularity\n\n\n\n\nSee \nInstalling\n for more info.\n\n\nIf you do not have an environment set up, the \n\nWorker Node Setup Walkthrough\n\ncovers setting up a worker node on AWS.\n\n\nClone The Repo\n\n\nStart by cloning a copy of dahak-taco:\n\n\n$ git clone https://github.com/dahak-metagenomics/dahak-taco.git\n$ cd dahak-taco/\n\n\n\n\nAs mentioned in \nInstalling\n,\nthere is nothing to install for taco itself.\n\n\nStart the Workflow\n\n\nFor the read filtering walkthrough we will be downloading\ntwo \n*.fq.gz\n files, assessing their quality,\nand trimming them based on the quality of the reads.\n\n\nThe workflow has four steps:\n\n\n\n\nDownload the data\n\n\nAssess the quality before trimming\n\n\nTrim the data\n\n\nAssess the quality after trimming\n\n\n\n\nPart 1: Download Data\n\n\nIn Part 1 we'll specify a filename as a target\nto download read data.\n\n\nOur three required inputs are:\n\n\n\n\nWorkflow name (\nread_filtering\n)\n\n\nWorkflow configuration file (in \ngoodies/readfilt1config.json\n)\n\n\nWorkflow parameters file (in \ngoodies/readfilt1params.json\n)\n\n\n\n\nHere is the workflow configuration file we will use.\nThe workflow targets it specifies are two files with \nunprocessed sequence reads.\n\n\n(These files are the inputs to the rest of the \nread filtering workflow.)\n\n\n$ cat goodies/readfilt1config.json\n{\n    \"short_description\": \"Read Filtering Walkthrough 1 - Configuration\",\n    \"workflow_targets\" : [\"data/SRR606249_1.fq.gz\",\n                          \"data/SRR606249_2.fq.gz\"]\n}\n\n\n\n\nNext, we have our workflow parameters file.\nThis parameters file should define any parameters\nused for the single step we're executing.\nIn our case, we have to tell Snakemake \nwhere to download the read files.\nThis parameters file defines a long list of files,\nbut rules are defined on a file-by-file basis\nso we only downlod the files we need:\n\n\n$ cat goodies/readfilt1params.json\n{\n    \"short_description\": \"Read Filtering Walkthrough 1 - Fetching Reads - Parameters\",\n    \"read_filtering\" : {\n        \"read_files\" : {\n            \"SRR606249_1.fq.gz\" :           \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f0f9156c613b026430dbc7\",\n            \"SRR606249_2.fq.gz\" :           \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f0fc7fb83f69026076be47\",\n            \"SRR606249_subset10_1.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f10134b83f69026377611b\",\n            \"SRR606249_subset10_2.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f101f26c613b026330e53a\",\n            \"SRR606249_subset25_1.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f1039a594d900263120c38\",\n            \"SRR606249_subset25_2.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f104ed594d90026411f486\",\n            \"SRR606249_subset50_1.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f1082d6c613b026430e5cf\",\n            \"SRR606249_subset50_2.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f10ac6594d900262123e77\"\n        },\n        \"read_patterns\" : {\n            \"pre_trimming_pattern\"  : \"{sample}_{direction}.fq.gz\"\n        }\n    }\n}\n\n\n\n\nBoth of these JSON files are contained in the \n\ngoodies/\n directory of the repo.\n\n\nThe \nread_patterns\n parameter is optional and the\ndefault value is shown here. This pattern is used\nto create the download wildcard rule, so the files \nthat make up the \nread_files\n keys must match that \npattern or they will not be downloadable.\n\n\nTo run the workflow defined by this workflow,\nconfig file, and parameters file, run taco as follows:\n\n\nDry run first with the \n-n\n flag:\n\n\n$ ./taco -n read_filtering \\\n    goodies/readfilt1config.json \\\n    goodies/readfilt1params.json\n\n\n\n\nThen the real deal:\n\n\n$ ./taco -n read_filtering \\\n    goodies/readfilt1config.json \\\n    goodies/readfilt1params.json\n\n\n\n\nPart 2: Perform Pre-Trim Quality Assessment\n\n\nIn Part 2 we'll perform a pre-trimming quality assessment\nby specifying filename targets.\n\n\nThe \nfastqc\n utility is used to assess the quality\nof the sequencer reads before any trimming occurs.\n\n\nThe file suffix for files whose quality has been\nassessed by fastqc is specified in the config file,\nor can be left as the default \n_fastqc\n.\n\n\nHere is the config file for the second step,\nwhich is the pre-trim quality assessment.\n\n\n$ cat goodies/readfilt2config.json\n{\n    \"short_description\": \"Read Filtering Walkthrough 1 - Fetching Reads - Configuration\",\n    \"workflow_targets\" : [\"data/SRR606249_1.fq.gz\",\n                          \"data/SRR606249_2.fq.gz\"]\n}\n\n\n\n\nWhile the second step of the workflow should use a \nsecond config file, the parameters file should \ninclude all the parameters from the prior step,\nsince Snakemake will still be running those workflow steps.\n\n\nHere is the parameters file:\n\n\n$ cat goodies/readfilt2params.json\n{\n    \"short_description\": \"Read Filtering Walkthrough 2 - Pre-Trim Quality Assessment - Parameters\",\n    \"biocontainers\" : {\n        \"fastqc\" : {\n            \"use_local\" : false,\n            \"quayurl\" : \"quay.io/biocontainers/fastqc\",\n            \"version\" : \"0.11.7--pl5.22.0_2\"\n        }\n    },\n    \"read_filtering\" : {\n        \"read_patterns\" : {\n            \"pre_trimming_pattern\"  : \"{sample}_{direction}_reads.fq.gz\"\n        },\n        \"quality_assessment\" : { \n            \"fastqc_suffix\" : \"fastqc\"\n        },\n        \"read_files\" : {\n            ...\n        }\n    }\n}\n\n\n\n\nThis uses the pre-trimming pattern to match filenames\ncontaining read files to assess.\n\n\nTo run the workflow defined by this workflow name,\nconfig file, and parameters file, run taco as follows:\n\n\nDry run first with the \n-n\n flag:\n\n\n$ ./taco -n read_filtering \\\n    goodies/readfilt2config.json \\\n    goodies/readfilt2params.json\n\n\n\n\nThen the real deal:\n\n\n$ ./taco read_filtering \\\n    goodies/readfilt2config.json \\\n    goodies/readfilt2params.json\n\n\n\n\nPart 3: Trim Reads\n\n\nIn Part 3 we will use trimmomatic to trim reads based on quality.\n(This will require us to re-run the prior steps.)\n\n\nIn Part 3, we will output the trimmed reads to a new file.\nTo do this, we must change the pre-trimming filename pattern,\nwhich is too general: \n{sample}_{direction}.fq.gz\n\nwill match nearly every \npost_trimming_pattern\n we choose.\n\n\nWe should change the pattern parameters to include \neither a prefix or a suffix:\n\n\n    \"pre_trimming_pattern\"  :  \"{sample}_{direction}_reads.fq.gz\"\n    \"post_trimming_pattern\"  : \"{sample}_{direction}_trim{qual}.fq.gz\"\n\n\n\n\nBecause we changed the target filenames for steps 1 and 2,\nthis will cause steps 1 and 2 to re-run.\n\n\nThis step also requires us to add an adapter file for \ntrimmomatic to use. This works like the read files: \nwe specify a target filename and a URL for the data.\n\n\nThe following step 3 config file explicitly specifies\ntargets for steps 1 and 2, in addition to step 3:\n\n\n$ cat goodies/readfilt3config.json\n{\n    \"short_description\": \"Read Filtering Walkthrough 3 - Trimming - Configuration\",\n    \"workflow_targets\":  [\"data/TruSeq2-PE.fa\",\n                          \"data/SRR606249_1_reads.fq.gz\",\n                          \"data/SRR606249_2_reads.fq.gz\",\n                          \"data/SRR606249_1_reads_fastqc.zip\",\n                          \"data/SRR606249_2_reads_fastqc.zip\",\n                          \"data/SRR606249_1_trim2.fq.gz\",\n                          \"data/SRR606249_2_trim2.fq.gz\"]\n}\n\n\n\n\nThe parameters file contains updated parameters\n(most options are not required, but this illustrates\nwhat controls the user has over file names):\n\n\n$ cat goodies/readfilt3params.json\n{\n    \"short_description\": \"Read Filtering Walkthrough 3 - Trimming - Parameters\",\n    \"biocontainers\" : {\n        \"trimmomatic\" : {\n            \"use_local\" : false,\n            \"quayurl\" : \"quay.io/biocontainers/trimmomatic\",\n            \"version\" : \"0.36--5\"\n        },\n        \"fastqc\" : {\n            \"use_local\" : false,\n            \"quayurl\" : \"quay.io/biocontainers/fastqc\",\n            \"version\" : \"0.11.7--pl5.22.0_2\"\n        }\n    },\n    \"read_filtering\" : {\n        \"read_patterns\" : {\n            \"pre_trimming_pattern\"  : \"{sample}_{direction}_reads.fq.gz\",\n            \"post_trimming_pattern\" : \"{sample}_{direction}_trim{qual}.fq.gz\"\n        },\n        \"quality_assessment\" : { \n            \"fastqc_suffix\" : \"fastqc\"\n        },\n        \"quality_trimming\" : {\n            \"trim_suffix\" : \"se\"\n        },\n        \"adapter_file\" : {\n            \"name\" : \"TruSeq2-PE.fa\",\n            \"url\"  : \"http://dib-training.ucdavis.edu.s3.amazonaws.com/mRNAseq-semi-2015-03-04/TruSeq2-PE.fa\"\n        },\n        \"read_files\" : {\n            ...\n        }\n    }\n}\n\n\n\n\nNote the \nread_files\n file names have been adjusted to match\nthe \npre_trimming_pattern\n.\n\n\nDry run first with the \n-n\n flag:\n\n\n$ ./taco -n read_filtering \\\n    goodies/readfilt3config.json \\\n    goodies/readfilt3params.json\n\n\n\n\nThen the real deal:\n\n\n$ ./taco read_filtering \\\n    goodies/readfilt3config.json \\\n    goodies/readfilt3params.json\n\n\n\n\nPart 3 (Modified): Trim Reads with Custom Docker Container\n\n\nEach step that utilizes a biocontainer from quay.io \ncan also be configured to use a local Docker image\nas well. \n\n\nCAVEATS:\n \n\n\n\n\nThis feature does not currently work for workflows\n    involving multiple machines (the container image\n    is not built and must already be available).\n\n\nThis workflow (use of local, customized docker files)\n    is intended as a last resort, or for development \n    purposes only.\n\n\n\n\nStart by building a \nfastqc\n docker container to use in lieu\nof the quay.io docker container defined in the default \nconfig dictionary. From the taco repository:\n\n\ncd docker_kludge/fastqc/\ndocker -t dahak_conda .\ncd ../../\n\n\n\n\nThis builds a container image called \ndahak_conda\n\nusing the Dockerfile in the given directory.\n\n\nNow add a \nbiocontainers\n section to the parameters \ndictionary. In the fastqc section, specify the name of the \nlocal container and specify that taco should use a \nlocal container.\n\n\nWe use the same config file as before, so that we run the \nsame workflow steps, but now the workflow is run using a \nlocal docker container.\n\n\nHere is the new file \nreadfilt3params_localdocker.json\n:\n\n\n$ cat readfilt3params_localdocker.json\n{\n    \"short_description\": \"Read Filtering Walkthrough 3 Modified - Trimming - Parameters\",\n    \"biocontainers\" : {\n        \"trimmomatic\" : {\n            \"use_local\" : false,\n            \"quayurl\" : \"quay.io/biocontainers/trimmomatic\",\n            \"version\" : \"0.36--5\"\n        },\n        \"fastqc\" : {\n            \"local\" : \"dahak_fastqc\",\n            \"use_local\" : true\n        }\n    },\n    \"read_filtering\" : {\n        ...\n    }\n}\n\n\n\n\nAnd here is the command to run the workflow:\n\n\n\n$ # Dry run\n\n$ ./taco -n read_filtering \\\n    goodies/readfilt3config.json \\\n    goodies/readfilt3params_localdocker.json\n\n$ # Real thing:\n\n$ ./taco -n read_filtering \\\n    goodies/readfilt3config.json \\\n    goodies/readfilt3params_localdocker.json\n\n\n\n\nSteps 4 and 5: Rule: Post-Trim Quality Assessment\n\n\nIn steps 4 and 5 we request the output file from the quality \nassessment pipeline. \n\n\nIn step 4, the first quality trimming, we'll use a quality value of 2.\n\n\nIn step 5, the second quality trimming, we'll use a quality value of 30.\n\n\nThe config file requests trimmed reads with a quality value of 2,\ndone using trimmomatic, and a fastqc quality-assessed output \nfile (fastqc.zip files).\n\n\n$ cat goodies/readfilt4config.json\n{\n    \"short_description\": \"Read Filtering Walkthrough 4 - Quality Filtering at 2 - Configuration\",\n    \"workflow_targets\" : [\"data/SRR606249_1_trim2.fq.gz\",\n                          \"data/SRR606249_2_trim2.fq.gz\",\n                          \"data/SRR606249_1_trim2_fastqc.zip\",\n                          \"data/SRR606249_2_trim2_fastqc.zip\"]\n}\n\n\n$ cat goodies/readfilt5config.json\n{\n    \"short_description\": \"Read Filtering Walkthrough 5 - Quality Filtering at 30 - Configuration\",\n    \"workflow_targets\" : [\"data/SRR606249_1_trim30.fq.gz\",\n                          \"data/SRR606249_2_trim30.fq.gz\",\n                          \"data/SRR606249_1_trim30_fastqc.zip\",\n                          \"data/SRR606249_2_trim30_fastqc.zip\"]\n}\n\n\n\n\nNext, the parameters file requires the user to \nspecify a few additional parameters \n\n\n$ cat goodies/readfilt4params.json\n{\n    \"short_description\": \"Read Filtering Walkthrough 4 - Quality Filtering - Parameters\",\n    \"biocontainers\" : {\n        ...\n    },\n    \"read_filtering\" : {\n        \"read_patterns\" : {\n            \"pre_trimming_pattern\"  : \"{sample}_{direction}_reads.fq.gz\",\n            \"post_trimming_pattern\" : \"{sample}_{direction}_trim{qual}.fq.gz\"\n        },\n        \"quality_assessment\" : { \n            \"fastqc_suffix\" : \"fastqc\"\n        },\n        \"quality_trimming\" : {\n            \"trim_suffix\" : \"se\"\n        },\n        \"adapter_file\" : {\n            \"name\" : \"TruSeq2-PE.fa\",\n            \"url\"  : \"http://dib-training.ucdavis.edu.s3.amazonaws.com/mRNAseq-semi-2015-03-04/TruSeq2-PE.fa\"\n        },\n        \"read_files\" : {\n            ...\n        }\n    }\n}\n\n\n\n\nNote that the parameters file for steps 4 and 5 will look identical, \nsince both run the same workflow and the only change is in the quality\nvalue (which is only specified by the filename).\n\n\nNow execute the commands that will run the workflow:\n\n\n\n$ # Dry run\n\n$ ./taco -n read_filtering \\\n    goodies/readfilt4config.json \\\n    goodies/readfilt4params.json\n\n$ ./taco -n read_filtering \\\n    goodies/readfilt5config.json \\\n    goodies/readfilt5params.json\n\n\n\n$ # Real thing:\n\n$ ./taco -n read_filtering \\\n    goodies/readfilt4config.json \\\n    goodies/readfilt4params.json\n\n$ ./taco -n read_filtering \\\n    goodies/readfilt5config.json \\\n    goodies/readfilt5params.json\n\n\n\n\nStep 6: Interleaving Trimmed Reads\n\n\nThe last step is to use khmer to interleave\ntrimmed reads after running trimmomatic and\nthe \nquality_trimming\n rule.\n\n\nThe interleaving step takes two input files,\nthe trimmed reads we want to use, \nrequires one additional parameter,\nwhich is the filename suffix we want \npaired-end reads to have \n(for example, \npe\n).\n\n\nThe config file uses this when specifying the target file:\n\n\n$ cat goodies/readfilt6config.json\n{\n    \"short_description\": \"Read Filtering Walkthrough 6 - Interleaving Trimmed Reads - Configuration\",\n    \"workflow_targets\" : [\"data/SRR606249_pe_trim2.fq.gz\",\n                          \"data/SRR606249_pe_trim2.fq.gz\",\n                          \"data/SRR606249_pe_trim30.fq.gz\",\n                          \"data/SRR606249_pe_trim30.fq.gz\"]\n}\n\n\n\n\nand the params file:\n\n\n$ cat goodies/readfilt6params.json\n{\n    \"short_description\": \"Read Filtering Walkthrough 6 - Interleaving Trimmed Reads - Parameters\",\n    \"biocontainers\" : {\n        \"trimmomatic\" : {\n            ...\n        },\n        \"fastqc\" : {\n            ...\n        },\n        \"khmer\" : {\n            \"use_local\" : false,\n            \"quayurl\" : \"quay.io/biocontainers/khmer\",\n            \"version\" : \"2.1.2--py35_0\"\n        }\n    },\n    \"read_filtering\" : {\n        ...\n        \"interleaving\" : {\n            \"interleave_suffix\" : \"pe\"\n        },\n        ...\n    }\n}\n\n\n\n\n$ # Dry run\n\n$ ./taco -n read_filtering \\\n    goodies/readfilt6config.json \\\n    goodies/readfilt6params.json\n\n\n\n$ # Real thing:\n\n$ ./taco -n read_filtering \\\n    goodies/readfilt6config.json \\\n    goodies/readfilt6params.json",
            "title": "Read Filtering Walkthrough"
        },
        {
            "location": "/walkthrus/ReadFiltWalkthru/#read-filtering-walkthrough",
            "text": "This walkthrough covers a read filtering workflow.   In this document we will:    Download raw sequence data from OSF data store (wget).    Assess the quality of the pre-filtered reads (fastqc)    Trim and filter the reads based on a quality threshold (trimmomatic)    Assess the quality of the post-filtered reads (fastqc)    Interleave post-filtered reads (khmer)     This workflow covers the use of taco to run the workflow.  See the  walkthroughs dir of the dahak repo \nfor the original shell-based walkthrough.",
            "title": "Read Filtering Walkthrough"
        },
        {
            "location": "/walkthrus/ReadFiltWalkthru/#setup",
            "text": "This node assumes you have the following software:   Python  Conda  Snakemake  Singularity   See  Installing  for more info.  If you do not have an environment set up, the  Worker Node Setup Walkthrough \ncovers setting up a worker node on AWS.",
            "title": "Setup"
        },
        {
            "location": "/walkthrus/ReadFiltWalkthru/#clone-the-repo",
            "text": "Start by cloning a copy of dahak-taco:  $ git clone https://github.com/dahak-metagenomics/dahak-taco.git\n$ cd dahak-taco/  As mentioned in  Installing ,\nthere is nothing to install for taco itself.",
            "title": "Clone The Repo"
        },
        {
            "location": "/walkthrus/ReadFiltWalkthru/#start-the-workflow",
            "text": "For the read filtering walkthrough we will be downloading\ntwo  *.fq.gz  files, assessing their quality,\nand trimming them based on the quality of the reads.  The workflow has four steps:   Download the data  Assess the quality before trimming  Trim the data  Assess the quality after trimming",
            "title": "Start the Workflow"
        },
        {
            "location": "/walkthrus/ReadFiltWalkthru/#part-1-download-data",
            "text": "In Part 1 we'll specify a filename as a target\nto download read data.  Our three required inputs are:   Workflow name ( read_filtering )  Workflow configuration file (in  goodies/readfilt1config.json )  Workflow parameters file (in  goodies/readfilt1params.json )   Here is the workflow configuration file we will use.\nThe workflow targets it specifies are two files with \nunprocessed sequence reads.  (These files are the inputs to the rest of the \nread filtering workflow.)  $ cat goodies/readfilt1config.json\n{\n    \"short_description\": \"Read Filtering Walkthrough 1 - Configuration\",\n    \"workflow_targets\" : [\"data/SRR606249_1.fq.gz\",\n                          \"data/SRR606249_2.fq.gz\"]\n}  Next, we have our workflow parameters file.\nThis parameters file should define any parameters\nused for the single step we're executing.\nIn our case, we have to tell Snakemake \nwhere to download the read files.\nThis parameters file defines a long list of files,\nbut rules are defined on a file-by-file basis\nso we only downlod the files we need:  $ cat goodies/readfilt1params.json\n{\n    \"short_description\": \"Read Filtering Walkthrough 1 - Fetching Reads - Parameters\",\n    \"read_filtering\" : {\n        \"read_files\" : {\n            \"SRR606249_1.fq.gz\" :           \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f0f9156c613b026430dbc7\",\n            \"SRR606249_2.fq.gz\" :           \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f0fc7fb83f69026076be47\",\n            \"SRR606249_subset10_1.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f10134b83f69026377611b\",\n            \"SRR606249_subset10_2.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f101f26c613b026330e53a\",\n            \"SRR606249_subset25_1.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f1039a594d900263120c38\",\n            \"SRR606249_subset25_2.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f104ed594d90026411f486\",\n            \"SRR606249_subset50_1.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f1082d6c613b026430e5cf\",\n            \"SRR606249_subset50_2.fq.gz\" :  \"files.osf.io/v1/resources/dm938/providers/osfstorage/59f10ac6594d900262123e77\"\n        },\n        \"read_patterns\" : {\n            \"pre_trimming_pattern\"  : \"{sample}_{direction}.fq.gz\"\n        }\n    }\n}  Both of these JSON files are contained in the  goodies/  directory of the repo.  The  read_patterns  parameter is optional and the\ndefault value is shown here. This pattern is used\nto create the download wildcard rule, so the files \nthat make up the  read_files  keys must match that \npattern or they will not be downloadable.  To run the workflow defined by this workflow,\nconfig file, and parameters file, run taco as follows:  Dry run first with the  -n  flag:  $ ./taco -n read_filtering \\\n    goodies/readfilt1config.json \\\n    goodies/readfilt1params.json  Then the real deal:  $ ./taco -n read_filtering \\\n    goodies/readfilt1config.json \\\n    goodies/readfilt1params.json",
            "title": "Part 1: Download Data"
        },
        {
            "location": "/walkthrus/ReadFiltWalkthru/#part-2-perform-pre-trim-quality-assessment",
            "text": "In Part 2 we'll perform a pre-trimming quality assessment\nby specifying filename targets.  The  fastqc  utility is used to assess the quality\nof the sequencer reads before any trimming occurs.  The file suffix for files whose quality has been\nassessed by fastqc is specified in the config file,\nor can be left as the default  _fastqc .  Here is the config file for the second step,\nwhich is the pre-trim quality assessment.  $ cat goodies/readfilt2config.json\n{\n    \"short_description\": \"Read Filtering Walkthrough 1 - Fetching Reads - Configuration\",\n    \"workflow_targets\" : [\"data/SRR606249_1.fq.gz\",\n                          \"data/SRR606249_2.fq.gz\"]\n}  While the second step of the workflow should use a \nsecond config file, the parameters file should \ninclude all the parameters from the prior step,\nsince Snakemake will still be running those workflow steps.  Here is the parameters file:  $ cat goodies/readfilt2params.json\n{\n    \"short_description\": \"Read Filtering Walkthrough 2 - Pre-Trim Quality Assessment - Parameters\",\n    \"biocontainers\" : {\n        \"fastqc\" : {\n            \"use_local\" : false,\n            \"quayurl\" : \"quay.io/biocontainers/fastqc\",\n            \"version\" : \"0.11.7--pl5.22.0_2\"\n        }\n    },\n    \"read_filtering\" : {\n        \"read_patterns\" : {\n            \"pre_trimming_pattern\"  : \"{sample}_{direction}_reads.fq.gz\"\n        },\n        \"quality_assessment\" : { \n            \"fastqc_suffix\" : \"fastqc\"\n        },\n        \"read_files\" : {\n            ...\n        }\n    }\n}  This uses the pre-trimming pattern to match filenames\ncontaining read files to assess.  To run the workflow defined by this workflow name,\nconfig file, and parameters file, run taco as follows:  Dry run first with the  -n  flag:  $ ./taco -n read_filtering \\\n    goodies/readfilt2config.json \\\n    goodies/readfilt2params.json  Then the real deal:  $ ./taco read_filtering \\\n    goodies/readfilt2config.json \\\n    goodies/readfilt2params.json",
            "title": "Part 2: Perform Pre-Trim Quality Assessment"
        },
        {
            "location": "/walkthrus/ReadFiltWalkthru/#part-3-trim-reads",
            "text": "In Part 3 we will use trimmomatic to trim reads based on quality.\n(This will require us to re-run the prior steps.)  In Part 3, we will output the trimmed reads to a new file.\nTo do this, we must change the pre-trimming filename pattern,\nwhich is too general:  {sample}_{direction}.fq.gz \nwill match nearly every  post_trimming_pattern  we choose.  We should change the pattern parameters to include \neither a prefix or a suffix:      \"pre_trimming_pattern\"  :  \"{sample}_{direction}_reads.fq.gz\"\n    \"post_trimming_pattern\"  : \"{sample}_{direction}_trim{qual}.fq.gz\"  Because we changed the target filenames for steps 1 and 2,\nthis will cause steps 1 and 2 to re-run.  This step also requires us to add an adapter file for \ntrimmomatic to use. This works like the read files: \nwe specify a target filename and a URL for the data.  The following step 3 config file explicitly specifies\ntargets for steps 1 and 2, in addition to step 3:  $ cat goodies/readfilt3config.json\n{\n    \"short_description\": \"Read Filtering Walkthrough 3 - Trimming - Configuration\",\n    \"workflow_targets\":  [\"data/TruSeq2-PE.fa\",\n                          \"data/SRR606249_1_reads.fq.gz\",\n                          \"data/SRR606249_2_reads.fq.gz\",\n                          \"data/SRR606249_1_reads_fastqc.zip\",\n                          \"data/SRR606249_2_reads_fastqc.zip\",\n                          \"data/SRR606249_1_trim2.fq.gz\",\n                          \"data/SRR606249_2_trim2.fq.gz\"]\n}  The parameters file contains updated parameters\n(most options are not required, but this illustrates\nwhat controls the user has over file names):  $ cat goodies/readfilt3params.json\n{\n    \"short_description\": \"Read Filtering Walkthrough 3 - Trimming - Parameters\",\n    \"biocontainers\" : {\n        \"trimmomatic\" : {\n            \"use_local\" : false,\n            \"quayurl\" : \"quay.io/biocontainers/trimmomatic\",\n            \"version\" : \"0.36--5\"\n        },\n        \"fastqc\" : {\n            \"use_local\" : false,\n            \"quayurl\" : \"quay.io/biocontainers/fastqc\",\n            \"version\" : \"0.11.7--pl5.22.0_2\"\n        }\n    },\n    \"read_filtering\" : {\n        \"read_patterns\" : {\n            \"pre_trimming_pattern\"  : \"{sample}_{direction}_reads.fq.gz\",\n            \"post_trimming_pattern\" : \"{sample}_{direction}_trim{qual}.fq.gz\"\n        },\n        \"quality_assessment\" : { \n            \"fastqc_suffix\" : \"fastqc\"\n        },\n        \"quality_trimming\" : {\n            \"trim_suffix\" : \"se\"\n        },\n        \"adapter_file\" : {\n            \"name\" : \"TruSeq2-PE.fa\",\n            \"url\"  : \"http://dib-training.ucdavis.edu.s3.amazonaws.com/mRNAseq-semi-2015-03-04/TruSeq2-PE.fa\"\n        },\n        \"read_files\" : {\n            ...\n        }\n    }\n}  Note the  read_files  file names have been adjusted to match\nthe  pre_trimming_pattern .  Dry run first with the  -n  flag:  $ ./taco -n read_filtering \\\n    goodies/readfilt3config.json \\\n    goodies/readfilt3params.json  Then the real deal:  $ ./taco read_filtering \\\n    goodies/readfilt3config.json \\\n    goodies/readfilt3params.json",
            "title": "Part 3: Trim Reads"
        },
        {
            "location": "/walkthrus/ReadFiltWalkthru/#part-3-modified-trim-reads-with-custom-docker-container",
            "text": "Each step that utilizes a biocontainer from quay.io \ncan also be configured to use a local Docker image\nas well.   CAVEATS:     This feature does not currently work for workflows\n    involving multiple machines (the container image\n    is not built and must already be available).  This workflow (use of local, customized docker files)\n    is intended as a last resort, or for development \n    purposes only.   Start by building a  fastqc  docker container to use in lieu\nof the quay.io docker container defined in the default \nconfig dictionary. From the taco repository:  cd docker_kludge/fastqc/\ndocker -t dahak_conda .\ncd ../../  This builds a container image called  dahak_conda \nusing the Dockerfile in the given directory.  Now add a  biocontainers  section to the parameters \ndictionary. In the fastqc section, specify the name of the \nlocal container and specify that taco should use a \nlocal container.  We use the same config file as before, so that we run the \nsame workflow steps, but now the workflow is run using a \nlocal docker container.  Here is the new file  readfilt3params_localdocker.json :  $ cat readfilt3params_localdocker.json\n{\n    \"short_description\": \"Read Filtering Walkthrough 3 Modified - Trimming - Parameters\",\n    \"biocontainers\" : {\n        \"trimmomatic\" : {\n            \"use_local\" : false,\n            \"quayurl\" : \"quay.io/biocontainers/trimmomatic\",\n            \"version\" : \"0.36--5\"\n        },\n        \"fastqc\" : {\n            \"local\" : \"dahak_fastqc\",\n            \"use_local\" : true\n        }\n    },\n    \"read_filtering\" : {\n        ...\n    }\n}  And here is the command to run the workflow:  \n$ # Dry run\n\n$ ./taco -n read_filtering \\\n    goodies/readfilt3config.json \\\n    goodies/readfilt3params_localdocker.json\n\n$ # Real thing:\n\n$ ./taco -n read_filtering \\\n    goodies/readfilt3config.json \\\n    goodies/readfilt3params_localdocker.json",
            "title": "Part 3 (Modified): Trim Reads with Custom Docker Container"
        },
        {
            "location": "/walkthrus/ReadFiltWalkthru/#steps-4-and-5-rule-post-trim-quality-assessment",
            "text": "In steps 4 and 5 we request the output file from the quality \nassessment pipeline.   In step 4, the first quality trimming, we'll use a quality value of 2.  In step 5, the second quality trimming, we'll use a quality value of 30.  The config file requests trimmed reads with a quality value of 2,\ndone using trimmomatic, and a fastqc quality-assessed output \nfile (fastqc.zip files).  $ cat goodies/readfilt4config.json\n{\n    \"short_description\": \"Read Filtering Walkthrough 4 - Quality Filtering at 2 - Configuration\",\n    \"workflow_targets\" : [\"data/SRR606249_1_trim2.fq.gz\",\n                          \"data/SRR606249_2_trim2.fq.gz\",\n                          \"data/SRR606249_1_trim2_fastqc.zip\",\n                          \"data/SRR606249_2_trim2_fastqc.zip\"]\n}\n\n\n$ cat goodies/readfilt5config.json\n{\n    \"short_description\": \"Read Filtering Walkthrough 5 - Quality Filtering at 30 - Configuration\",\n    \"workflow_targets\" : [\"data/SRR606249_1_trim30.fq.gz\",\n                          \"data/SRR606249_2_trim30.fq.gz\",\n                          \"data/SRR606249_1_trim30_fastqc.zip\",\n                          \"data/SRR606249_2_trim30_fastqc.zip\"]\n}  Next, the parameters file requires the user to \nspecify a few additional parameters   $ cat goodies/readfilt4params.json\n{\n    \"short_description\": \"Read Filtering Walkthrough 4 - Quality Filtering - Parameters\",\n    \"biocontainers\" : {\n        ...\n    },\n    \"read_filtering\" : {\n        \"read_patterns\" : {\n            \"pre_trimming_pattern\"  : \"{sample}_{direction}_reads.fq.gz\",\n            \"post_trimming_pattern\" : \"{sample}_{direction}_trim{qual}.fq.gz\"\n        },\n        \"quality_assessment\" : { \n            \"fastqc_suffix\" : \"fastqc\"\n        },\n        \"quality_trimming\" : {\n            \"trim_suffix\" : \"se\"\n        },\n        \"adapter_file\" : {\n            \"name\" : \"TruSeq2-PE.fa\",\n            \"url\"  : \"http://dib-training.ucdavis.edu.s3.amazonaws.com/mRNAseq-semi-2015-03-04/TruSeq2-PE.fa\"\n        },\n        \"read_files\" : {\n            ...\n        }\n    }\n}  Note that the parameters file for steps 4 and 5 will look identical, \nsince both run the same workflow and the only change is in the quality\nvalue (which is only specified by the filename).  Now execute the commands that will run the workflow:  \n$ # Dry run\n\n$ ./taco -n read_filtering \\\n    goodies/readfilt4config.json \\\n    goodies/readfilt4params.json\n\n$ ./taco -n read_filtering \\\n    goodies/readfilt5config.json \\\n    goodies/readfilt5params.json\n\n\n\n$ # Real thing:\n\n$ ./taco -n read_filtering \\\n    goodies/readfilt4config.json \\\n    goodies/readfilt4params.json\n\n$ ./taco -n read_filtering \\\n    goodies/readfilt5config.json \\\n    goodies/readfilt5params.json",
            "title": "Steps 4 and 5: Rule: Post-Trim Quality Assessment"
        },
        {
            "location": "/walkthrus/ReadFiltWalkthru/#step-6-interleaving-trimmed-reads",
            "text": "The last step is to use khmer to interleave\ntrimmed reads after running trimmomatic and\nthe  quality_trimming  rule.  The interleaving step takes two input files,\nthe trimmed reads we want to use, \nrequires one additional parameter,\nwhich is the filename suffix we want \npaired-end reads to have \n(for example,  pe ).  The config file uses this when specifying the target file:  $ cat goodies/readfilt6config.json\n{\n    \"short_description\": \"Read Filtering Walkthrough 6 - Interleaving Trimmed Reads - Configuration\",\n    \"workflow_targets\" : [\"data/SRR606249_pe_trim2.fq.gz\",\n                          \"data/SRR606249_pe_trim2.fq.gz\",\n                          \"data/SRR606249_pe_trim30.fq.gz\",\n                          \"data/SRR606249_pe_trim30.fq.gz\"]\n}  and the params file:  $ cat goodies/readfilt6params.json\n{\n    \"short_description\": \"Read Filtering Walkthrough 6 - Interleaving Trimmed Reads - Parameters\",\n    \"biocontainers\" : {\n        \"trimmomatic\" : {\n            ...\n        },\n        \"fastqc\" : {\n            ...\n        },\n        \"khmer\" : {\n            \"use_local\" : false,\n            \"quayurl\" : \"quay.io/biocontainers/khmer\",\n            \"version\" : \"2.1.2--py35_0\"\n        }\n    },\n    \"read_filtering\" : {\n        ...\n        \"interleaving\" : {\n            \"interleave_suffix\" : \"pe\"\n        },\n        ...\n    }\n}  $ # Dry run\n\n$ ./taco -n read_filtering \\\n    goodies/readfilt6config.json \\\n    goodies/readfilt6params.json\n\n\n\n$ # Real thing:\n\n$ ./taco -n read_filtering \\\n    goodies/readfilt6config.json \\\n    goodies/readfilt6params.json",
            "title": "Step 6: Interleaving Trimmed Reads"
        },
        {
            "location": "/ReadFiltering/",
            "text": "Workflow: \nread_filtering\n\n\nThe \nread_filtering\n workflow uses \n\nfastq\n and \ntrimmomatic\n to assess\nthe quality of sequencer reads and \nfilter out low-quality reads.\n\n\nThe user has control over the names \nand URLs of sequence data that is \ndownloaded, as well as parameters like\nthe quality threshold.",
            "title": "Read Filtering Workflow Parameters"
        },
        {
            "location": "/ReadFiltering/#workflow-read_filtering",
            "text": "The  read_filtering  workflow uses  fastq  and  trimmomatic  to assess\nthe quality of sequencer reads and \nfilter out low-quality reads.  The user has control over the names \nand URLs of sequence data that is \ndownloaded, as well as parameters like\nthe quality threshold.",
            "title": "Workflow: read_filtering"
        },
        {
            "location": "/dev/HowItWorks/",
            "text": "How Taco Works\n\n\nThe Tao of Taco\n\n\ntaco is a lightweight wrapper around Snakemake tasks.\nKind of like a corn tortilla.\n\n\n\n\nAll code is located in the \ntaco\n executable script\n\n\nIt is a short and simple command line interface implemented with argparser\n\n\nSnakemake is called via its Python API. \n\n\nThe user specifies which snakemake rule to call, and any parameters to use.\n\n\n\n\nUser Input\n\n\nUser input required to run taco:\n\n\n\n\nWorkflow config json file calls rules defined in \nrules/dahak/*.rules\n\n\nParameters json file overrides parameters defined in \nrules/dahak/*.settings\n\n\n\n\nSee \nIntroduction\n for a user-oriented introduction\nto workflow configuration and parameters.\n\n\nParameter Presets (Not Implemented)\n\n\nWe want to define a few command line options\nthat will be passed through the Snakemake \nAPI (via a configuration dictionary)\nto provide parameter presets.\n\n\n$ taco <workflow-name> \\\n    --workflow-preset=heavy \\\n    <config-json-file> \\\n    <params-json-file>\n\n\n\n\nSetting Variables from Command Line (Not Implemented)\n\n\n(Not implemented yet.)\n\n\nSetting variables:\n\n\nFor the user's convenience, we want to provide the user a way to \nchange/set parameter values on the command line. \n\n\nProvide a command line flag for simpler/common options.\nThis is flat, so you have to hard-code how the command line options\nmap to the particular JSON entry you want it to change.\n\n\n$ taco <workflow-name> \\\n    --quality=2,30,50 \\\n    --custom-param=custom_value \\\n    <config-json-file> \\\n    <params-json-file>",
            "title": "How It Works"
        },
        {
            "location": "/dev/HowItWorks/#how-taco-works",
            "text": "",
            "title": "How Taco Works"
        },
        {
            "location": "/dev/HowItWorks/#the-tao-of-taco",
            "text": "taco is a lightweight wrapper around Snakemake tasks.\nKind of like a corn tortilla.   All code is located in the  taco  executable script  It is a short and simple command line interface implemented with argparser  Snakemake is called via its Python API.   The user specifies which snakemake rule to call, and any parameters to use.",
            "title": "The Tao of Taco"
        },
        {
            "location": "/dev/HowItWorks/#user-input",
            "text": "User input required to run taco:   Workflow config json file calls rules defined in  rules/dahak/*.rules  Parameters json file overrides parameters defined in  rules/dahak/*.settings   See  Introduction  for a user-oriented introduction\nto workflow configuration and parameters.",
            "title": "User Input"
        },
        {
            "location": "/dev/HowItWorks/#parameter-presets-not-implemented",
            "text": "We want to define a few command line options\nthat will be passed through the Snakemake \nAPI (via a configuration dictionary)\nto provide parameter presets.  $ taco <workflow-name> \\\n    --workflow-preset=heavy \\\n    <config-json-file> \\\n    <params-json-file>",
            "title": "Parameter Presets (Not Implemented)"
        },
        {
            "location": "/dev/HowItWorks/#setting-variables-from-command-line-not-implemented",
            "text": "(Not implemented yet.)  Setting variables:  For the user's convenience, we want to provide the user a way to \nchange/set parameter values on the command line.   Provide a command line flag for simpler/common options.\nThis is flat, so you have to hard-code how the command line options\nmap to the particular JSON entry you want it to change.  $ taco <workflow-name> \\\n    --quality=2,30,50 \\\n    --custom-param=custom_value \\\n    <config-json-file> \\\n    <params-json-file>",
            "title": "Setting Variables from Command Line (Not Implemented)"
        },
        {
            "location": "/dev/SnakemakeRules/",
            "text": "Snakemake Rules\n\n\nA brief summary of how Snakemake rules are implemented and organized.\n\n\nWe follow the conventions set in the \nsnakemake-rules\n\nrepository, and groups all rules into \n.rule\n files and all parameters into\n\n.settings\n files.\n\n\nWorkflows\n\n\nThe workflows define a set of tasks.\nEach workflow is a sub-directory in the \nrules/\n \ndirectory.\n\n\nRule Files\n\n\nRule files define how steps in the workflow proceed.\nThey preprocess configuration variables.\nRules are defined in \n.rule\n files\nin the workflow directory.\n\n\nDefault Parameter Files\n\n\nThe default parameter dictionary is contained\nin a \n.settings\n file in the workflow directory.\nThese will \nNOT\n overwrite any parameters\nthat have already been set by the user in their\nJSON parameter file.\n\n\nValidation of Parameters\n\n\nBecause we don't know what workflow is being run\nwhen we process the parameters, we can't do much\nparameter validation.\n\n\nCurrently, we do not throw an exception \nwhen a parameter is undefined by the user,\nwe either silently use the default value \n(default behavior) or we silently set to \nan empty string (using \n--clean\n or \n-c\n\nflag).\n\n\nDocumentation of Snakemake Rules\n\n\nSnakemake rules are documented\nin the docstring of the Snakemake rule. \nFor example:\n\n\nrule do_stuff:\n    \"\"\"\n    Useful information about the .settings files\n    that do_stuff imports, or parameter values\n    that are important for the do_stuff rule,\n    can be added here.\n    \"\"\"\n\n    ...",
            "title": "Snakemake Rules"
        },
        {
            "location": "/dev/SnakemakeRules/#snakemake-rules",
            "text": "A brief summary of how Snakemake rules are implemented and organized.  We follow the conventions set in the  snakemake-rules \nrepository, and groups all rules into  .rule  files and all parameters into .settings  files.",
            "title": "Snakemake Rules"
        },
        {
            "location": "/dev/SnakemakeRules/#workflows",
            "text": "The workflows define a set of tasks.\nEach workflow is a sub-directory in the  rules/  \ndirectory.",
            "title": "Workflows"
        },
        {
            "location": "/dev/SnakemakeRules/#rule-files",
            "text": "Rule files define how steps in the workflow proceed.\nThey preprocess configuration variables.\nRules are defined in  .rule  files\nin the workflow directory.",
            "title": "Rule Files"
        },
        {
            "location": "/dev/SnakemakeRules/#default-parameter-files",
            "text": "The default parameter dictionary is contained\nin a  .settings  file in the workflow directory.\nThese will  NOT  overwrite any parameters\nthat have already been set by the user in their\nJSON parameter file.",
            "title": "Default Parameter Files"
        },
        {
            "location": "/dev/SnakemakeRules/#validation-of-parameters",
            "text": "Because we don't know what workflow is being run\nwhen we process the parameters, we can't do much\nparameter validation.  Currently, we do not throw an exception \nwhen a parameter is undefined by the user,\nwe either silently use the default value \n(default behavior) or we silently set to \nan empty string (using  --clean  or  -c \nflag).",
            "title": "Validation of Parameters"
        },
        {
            "location": "/dev/SnakemakeRules/#documentation-of-snakemake-rules",
            "text": "Snakemake rules are documented\nin the docstring of the Snakemake rule. \nFor example:  rule do_stuff:\n    \"\"\"\n    Useful information about the .settings files\n    that do_stuff imports, or parameter values\n    that are important for the do_stuff rule,\n    can be added here.\n    \"\"\"\n\n    ...",
            "title": "Documentation of Snakemake Rules"
        },
        {
            "location": "/dev/Workflows/",
            "text": "Adding New Taco Workflows\n\n\nIf you wish to extend or modify taco by \nadding a new workflow, you can do so,\nbut it's important to plan it out first!\n\n\nEach Snakemake workflow is composed of \na set of loosely-interconnected rules \nsharing parameters and handling pieces \nof the whole workflow.\n\n\nThe workflow lives in a directory \nin \nrules\n that is named after the \nworkflow. For example, to add a new\nworkflow called \nfoobar\n:\n\n\nrules/\n    foobar/\n    taxonomic_classification/\n    read_filtering/\n    functional_annotation/\n    ...\n\n\n\n\nEach rule or group of rules lives in a \n.rule file. All rule files have access\nto the entire configuration dictionary,\nwhich stores all workflow parameters.\n\n\nSuppose our new workflow \nfoobar\n had \nthree distinct steps: \nbuz\n, \nfuz\n, and \nwuz\n.\nThen we would create a rule file for each,\nand the \nfoobar/\n rules directory structure\nwould be:\n\n\nrules/\n    foobar/\n        buz.rule\n        fuz.rule\n        wuz.rule\n        foobar.settings\n\n\n\n\nAdditionally, we need to set parameters for the workflow.\n\n\nEach workflow has access to the \"master\" Snakemake\nparameters dictionary. \n\n\nIn addition, each workflow defines its own parameters dictionary\nto store parameters specific to that workflow.\n\n\nThis parameters dictionary is stored under a key that is the name \nof the workflow.\n\n\nFor example, if each of our three workflow steps took \nparameters, here is how we would organize the \nworkflow's default parameters dictionary:\n\n\n{\n    ...\n\n    'foobar' : {\n\n        'buz' : {\n            'buz_param_1' : 1,\n            'buz_param_2' : 'alpha'\n        },\n\n        'fuz' : {\n            'fuz_param_1' : 50,\n            'fuz_param_2' : 51,\n            'fuz_param_3' : False\n        },\n\n        'wuz' : {\n            'wuz_param_1' : 9.99,\n            'wuz_param_2' : 9.98,\n            'wuz_param_3' : 9.97,\n            'wuz_param_4' : 9.96\n        }\n    }\n    ...\n}\n\n\n\n\nThe \nfoobar.settings\n file must set this \nin a way that will not overwrite defaults;\nhence this business:\n\n\nfrom snakemake.utils import update_config\n\nif(not config['clean']):\n\n    # Note: don't include http:// or https://\n    config_default = { \n                        ... \n                     }\n\n    update_config(config_default, config)\n    config = config_default",
            "title": "Workflows"
        },
        {
            "location": "/dev/Workflows/#adding-new-taco-workflows",
            "text": "If you wish to extend or modify taco by \nadding a new workflow, you can do so,\nbut it's important to plan it out first!  Each Snakemake workflow is composed of \na set of loosely-interconnected rules \nsharing parameters and handling pieces \nof the whole workflow.  The workflow lives in a directory \nin  rules  that is named after the \nworkflow. For example, to add a new\nworkflow called  foobar :  rules/\n    foobar/\n    taxonomic_classification/\n    read_filtering/\n    functional_annotation/\n    ...  Each rule or group of rules lives in a \n.rule file. All rule files have access\nto the entire configuration dictionary,\nwhich stores all workflow parameters.  Suppose our new workflow  foobar  had \nthree distinct steps:  buz ,  fuz , and  wuz .\nThen we would create a rule file for each,\nand the  foobar/  rules directory structure\nwould be:  rules/\n    foobar/\n        buz.rule\n        fuz.rule\n        wuz.rule\n        foobar.settings  Additionally, we need to set parameters for the workflow.  Each workflow has access to the \"master\" Snakemake\nparameters dictionary.   In addition, each workflow defines its own parameters dictionary\nto store parameters specific to that workflow.  This parameters dictionary is stored under a key that is the name \nof the workflow.  For example, if each of our three workflow steps took \nparameters, here is how we would organize the \nworkflow's default parameters dictionary:  {\n    ...\n\n    'foobar' : {\n\n        'buz' : {\n            'buz_param_1' : 1,\n            'buz_param_2' : 'alpha'\n        },\n\n        'fuz' : {\n            'fuz_param_1' : 50,\n            'fuz_param_2' : 51,\n            'fuz_param_3' : False\n        },\n\n        'wuz' : {\n            'wuz_param_1' : 9.99,\n            'wuz_param_2' : 9.98,\n            'wuz_param_3' : 9.97,\n            'wuz_param_4' : 9.96\n        }\n    }\n    ...\n}  The  foobar.settings  file must set this \nin a way that will not overwrite defaults;\nhence this business:  from snakemake.utils import update_config\n\nif(not config['clean']):\n\n    # Note: don't include http:// or https://\n    config_default = { \n                        ... \n                     }\n\n    update_config(config_default, config)\n    config = config_default",
            "title": "Adding New Taco Workflows"
        },
        {
            "location": "/dev/Docs/",
            "text": "Documentation\n\n\nMaking documentation:\n\n\ncd docs/\nmake html\ncd build/html\npython -m SimpleHTTPServer 8080\n\n\n\n\nNow navigate to \nlocalhost:8080\n in your bowser. Voila!",
            "title": "Documentation"
        },
        {
            "location": "/dev/Docs/#documentation",
            "text": "Making documentation:  cd docs/\nmake html\ncd build/html\npython -m SimpleHTTPServer 8080  Now navigate to  localhost:8080  in your bowser. Voila!",
            "title": "Documentation"
        }
    ]
}